{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Tryouts with Text file #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Function to encode the text\n",
    "def encode_text(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Input text\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attention masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "# PyTorch Dataset\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        utterance = item['utterance']\n",
    "        sarcasm = int(item['sarcasm'])\n",
    "        input_ids, attention_mask = encode_text(utterance)\n",
    "        return input_ids.flatten(), attention_mask.flatten(), sarcasm\n",
    "# Create the DataLoader\n",
    "# Load the data from the JSON file\n",
    "with open('sarcasm_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the data to a list of dictionaries\n",
    "data = list(data.values())\n",
    "\n",
    "dataset = SarcasmDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "#dataset = SarcasmDataset('sarcasm_data.json')\n",
    "#dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_preds = []\n",
    "    epoch_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            # Get the loss and logits from the outputs\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Add the loss to the epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Get the predictions\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            # Add the predictions and labels to the epoch predictions and labels\n",
    "            epoch_preds.extend(preds.cpu().numpy())\n",
    "            epoch_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute the metrics\n",
    "    loss = epoch_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(epoch_labels, epoch_preds)\n",
    "    f1 = f1_score(epoch_labels, epoch_preds, average='weighted')\n",
    "\n",
    "    model.train()  # Put the model back in training mode\n",
    "\n",
    "    return loss, accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/5:   9%|â–‰         | 2/22 [00:26<04:20, 13.01s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the base BERT model\n",
    "    num_labels = 2, # Binary classification (sarcasm or not)\n",
    "    output_attentions = False, # Whether the model returns attentions weights\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states\n",
    ")\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Move the model to the CPU\n",
    "model.to(device)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Initialize lists to store metrics for each epoch\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "train_f1_scores = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_f1_scores = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0  # Initialize loss for this epoch\n",
    "    epoch_preds = []  # Initialize predictions for this epoch\n",
    "    epoch_labels = []  # Initialize labels for this epoch\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        # Move batch to device\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # Get the loss and logits from the outputs\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Add the loss to the epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Add the predictions and labels to the epoch predictions and labels\n",
    "        epoch_preds.extend(preds.cpu().numpy())\n",
    "        epoch_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "    # After each epoch, compute the metrics and append them to their respective lists\n",
    "    train_losses.append(epoch_loss / len(dataloader))\n",
    "    train_accuracies.append(accuracy_score(epoch_labels, epoch_preds))\n",
    "    train_f1_scores.append(f1_score(epoch_labels, epoch_preds, average='weighted'))\n",
    "\n",
    "    # Evaluate on test data\n",
    "    test_loss, test_accuracy, test_f1 = evaluate(model, dataset, device)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_f1_scores.append(test_f1)\n",
    "\n",
    "# After all epochs, plot the metrics\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_f1_scores, label='Train F1 Score')\n",
    "plt.plot(test_f1_scores, label='Test F1 Score')\n",
    "plt.title('F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the base BERT model\n",
    "    num_labels = 2, # Binary classification (sarcasm or not)\n",
    "    output_attentions = False, # Whether the model returns attentions weights\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states\n",
    ")\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Move the model to the CPU\n",
    "model.to(device)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Initialize lists to store losses for each epoch\n",
    "train_losses = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0  # Initialize loss for this epoch\n",
    "    for batch in dataloader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # Get the loss from the outputs\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Add the loss to the epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "    # After each epoch, append the average epoch loss to train_losses\n",
    "    train_losses.append(epoch_loss / len(dataloader))\n",
    "\n",
    "# After all epochs, plot the training loss\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9826086956521739\n"
     ]
    }
   ],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Variables to keep track of predictions and true labels\n",
    "total_preds, total_labels = [], []\n",
    "\n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        # Add the predictions and the true labels to the total\n",
    "        total_preds.extend(preds.cpu().numpy())\n",
    "        total_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = (np.array(total_preds) == np.array(total_labels)).mean()\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm #to see the progress of training\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Function to encode the text\n",
    "def encode_text(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Input text\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        #pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attention masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "# PyTorch Dataset\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        utterance = item['utterance']\n",
    "        sarcasm = int(item['sarcasm'])\n",
    "        input_ids, attention_mask = encode_text(utterance)\n",
    "        return input_ids[0], attention_mask[0], torch.tensor(sarcasm)\n",
    "        \n",
    "# Create the DataLoader\n",
    "# Load the data from the JSON file\n",
    "with open('sarcasm_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the data to a list of dictionaries\n",
    "data = list(data.values())\n",
    "\n",
    "dataset = SarcasmDataset(data)\n",
    "\n",
    "    \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, attention_mask, labels = zip(*batch)\n",
    "    input_ids = pad_sequence([torch.tensor(b) for b in input_ids], batch_first=True)\n",
    "    attention_mask = pad_sequence([torch.tensor(b) for b in attention_mask], batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "#dataloader = DataLoader(dataset, batch_size=32)\n",
    "#dataset = SarcasmDataset('sarcasm_data.json')\n",
    "#dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def f1(preds, target):\n",
    "    return f1_score(target, preds, average='macro')\n",
    "\n",
    "def acc(preds, target):\n",
    "    return accuracy_score(target, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, optimizer, criterion, metrics, train_loader, device):\n",
    "    '''\n",
    "    device = torch.device('cuda') or torch.device('cpu') if no GPU available\n",
    "    '''\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "\n",
    "    for batch_num, (input_ids, attention_mask, y_batch) in tqdm(enumerate(train_loader)):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        target = y_batch.to(device)\n",
    "        #clean previously computed gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #compute predictions\n",
    "        with torch.no_grad():\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(outputs.logits, target)\n",
    "\n",
    "        #do backward and oprimizer steps\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute metrics\n",
    "        # no gradients should be propagated at this step\n",
    "        with torch.no_grad():\n",
    "            for k in epoch_metrics.keys():\n",
    "                epoch_metrics[k] += metrics[k](preds, target)\n",
    "\n",
    "\n",
    "        # log loss statistics\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "\n",
    "    for k in epoch_metrics.keys():\n",
    "          epoch_metrics[k] /= len(train_loader)\n",
    "\n",
    "    clear_output() #clean the prints from previous epochs\n",
    "    print('train Loss: {:.4f}, '.format(epoch_loss),\n",
    "          ', '.join(['{}: {:.4f}'.format(k, epoch_metrics[k]) for k in epoch_metrics.keys()]))\n",
    "\n",
    "    return epoch_loss,  epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, metrics, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "\n",
    "    for batch_num, (input_ids, attention_mask, y_batch) in tqdm(enumerate(test_loader)):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        target = y_batch.to(device)\n",
    "\n",
    "        #forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        #compute predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # compute metrics\n",
    "        for k in epoch_metrics.keys():\n",
    "            epoch_metrics[k] += metrics[k](preds, target)\n",
    "\n",
    "        # log loss statistics\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "\n",
    "    epoch_loss /= len(test_loader)\n",
    "\n",
    "    for k in epoch_metrics.keys():\n",
    "          epoch_metrics[k] /= len(test_loader)\n",
    "\n",
    "    print('eval Loss: {:.4f}, '.format(epoch_loss),\n",
    "          ', '.join(['{}: {:.4f}'.format(k, epoch_metrics[k]) for k in epoch_metrics.keys()]))\n",
    "\n",
    "    return epoch_loss,  epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_loss, test_loss, metrics_names, train_metrics_logs, test_metrics_logs):\n",
    "    fig, ax = plt.subplots(1, len(metrics_names) + 1, figsize=((len(metrics_names) + 1) * 5, 5))\n",
    "\n",
    "    ax[0].plot(train_loss, c='blue', label='train')\n",
    "    ax[0].plot(test_loss, c='orange', label='test')\n",
    "    ax[0].set_title('Loss')\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].legend()\n",
    "\n",
    "    for i in range(len(metrics_names)):\n",
    "        ax[i + 1].plot(train_metrics_logs[i], c='blue', label='train')\n",
    "        ax[i + 1].plot(test_metrics_logs[i], c='orange', label='test')\n",
    "        ax[i + 1].set_title(metrics_names[i])\n",
    "        ax[i + 1].set_xlabel('epoch')\n",
    "        ax[i + 1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually you have to use validation for each step of training, but now we will focus only on the toy example and will track the perfromance on test\n",
    "def update_metrics_log(metrics_names, metrics_log, new_metrics_dict):\n",
    "    for i in range(len(metrics_names)):\n",
    "        curr_metric_name = metrics_names[i]\n",
    "        metrics_log[i].append(new_metrics_dict[curr_metric_name])\n",
    "    return metrics_log\n",
    "\n",
    "\n",
    "def train_cycle(model, optimizer, criterion, metrics, train_loader, test_loader, n_epochs, device):\n",
    "    train_loss_log,  test_loss_log = [], []\n",
    "    metrics_names = list(metrics.keys())\n",
    "    train_metrics_log = [[] for i in range(len(metrics))]\n",
    "    test_metrics_log = [[] for i in range(len(metrics))]\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {0} of {1}\".format(epoch, n_epochs))\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_loader, device)\n",
    "\n",
    "        test_loss, test_metrics = evaluate(model, criterion, metrics, test_loader, device)\n",
    "\n",
    "        train_loss_log.append(train_loss)\n",
    "        train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "\n",
    "        test_loss_log.append(test_loss)\n",
    "        test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, test_metrics)\n",
    "\n",
    "        plot_training(train_loss_log, test_loss_log, metrics_names, train_metrics_log, test_metrics_log)\n",
    "    return train_metrics_log, test_metrics_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually you have to use validation for each step of training, but now we will focus only on the toy example and will track the perfromance on test\n",
    "def update_metrics_log(metrics_names, metrics_log, new_metrics_dict):\n",
    "    for i in range(len(metrics_names)):\n",
    "        curr_metric_name = metrics_names[i]\n",
    "        metrics_log[i].append(new_metrics_dict[curr_metric_name])\n",
    "    return metrics_log\n",
    "\n",
    "\n",
    "def train_cycle(model, optimizer, criterion, metrics, train_loader, test_loader, n_epochs, device):\n",
    "    train_loss_log,  test_loss_log = [], []\n",
    "    metrics_names = list(metrics.keys())\n",
    "    train_metrics_log = [[] for i in range(len(metrics))]\n",
    "    test_metrics_log = [[] for i in range(len(metrics))]\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {0} of {1}\".format(epoch, n_epochs))\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_loader, device)\n",
    "\n",
    "        test_loss, test_metrics = evaluate(model, criterion, metrics, test_loader, device)\n",
    "\n",
    "        train_loss_log.append(train_loss)\n",
    "        train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "\n",
    "        test_loss_log.append(test_loss)\n",
    "        test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, test_metrics)\n",
    "\n",
    "        plot_training(train_loss_log, test_loss_log, metrics_names, train_metrics_log, test_metrics_log)\n",
    "    return train_metrics_log, test_metrics_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0137,  ACC: 0.5909, F1-weighted: 0.3907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m N_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#train_metrics_log, test_metrics_log = train_cycle(model, optimizer, torch.nn.CrossEntropyLoss(), {'accuracy': torch.nn.functional.accuracy}, dataloader, dataloader, 5, device)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m train_metrics_log, test_metrics_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 20\u001b[0m, in \u001b[0;36mtrain_cycle\u001b[0;34m(model, optimizer, criterion, metrics, train_loader, test_loader, n_epochs, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, n_epochs))\n\u001b[1;32m     18\u001b[0m train_loss, train_metrics \u001b[38;5;241m=\u001b[39m train_epoch(model, optimizer, criterion, metrics, train_loader, device)\n\u001b[0;32m---> 20\u001b[0m test_loss, test_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m train_loss_log\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     23\u001b[0m train_metrics_log \u001b[38;5;241m=\u001b[39m update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, criterion, metrics, test_loader, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m target \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#forward\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#compute loss\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DLproj/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DLproj/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DLproj/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DLproj/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DLproj/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DLproj/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:967\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 967\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m    968\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the base BERT model\n",
    "    num_labels = 2, # Binary classification (sarcasm or not)\n",
    "    output_attentions = False, # Whether the model returns attentions weights\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states\n",
    ")\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Move the model to the CPU\n",
    "model.to(device)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "#train_metrics_log, test_metrics_log = train_cycle(model, optimizer, torch.nn.CrossEntropyLoss(), {'accuracy': torch.nn.functional.accuracy}, dataloader, dataloader, 5, device)\n",
    "train_metrics_log, test_metrics_log = train_cycle(model, optimizer, criterion, metrics, dataloader, dataset, n_epochs=N_EPOCHS, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is supposed to work to! just does not show the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Function to encode the text\n",
    "def encode_text(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Input text\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attention masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "# PyTorch Dataset\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        utterance = item['utterance']\n",
    "        sarcasm = int(item['sarcasm'])\n",
    "        input_ids, attention_mask = encode_text(utterance)\n",
    "        return input_ids.flatten(), attention_mask.flatten(), sarcasm\n",
    "# Create the DataLoader\n",
    "# Load the data from the JSON file\n",
    "with open('sarcasm_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the data to a list of dictionaries\n",
    "data = list(data.values())\n",
    "\n",
    "dataset = SarcasmDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "#dataset = SarcasmDataset('sarcasm_data.json')\n",
    "#dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sinarollin/opt/anaconda3/envs/DLproj/lib/python3.11/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/sinarollin/opt/anaconda3/envs/DLproj/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the base BERT model\n",
    "    num_labels = 2, # Binary classification (sarcasm or not)\n",
    "    output_attentions = False, # Whether the model returns attentions weights\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states\n",
    ")\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Move the model to the CPU\n",
    "model.to(device)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # Get the loss from the outputs\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.863768115942029\n"
     ]
    }
   ],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Variables to keep track of predictions and true labels\n",
    "total_preds, total_labels = [], []\n",
    "\n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        # Add the predictions and the true labels to the total\n",
    "        total_preds.extend(preds.cpu().numpy())\n",
    "        total_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = (np.array(total_preds) == np.array(total_labels)).mean()\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
