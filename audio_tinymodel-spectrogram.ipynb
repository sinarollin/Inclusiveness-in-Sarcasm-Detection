{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 18:23:59.961665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Importing the necessary libraries\n",
    "from functions_audio_model_tiny import *\n",
    "from moviepy.editor import VideoFileClip\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, spectrogram_dir, labels):\n",
    "        self.spectrogram_dir = spectrogram_dir\n",
    "        self.file_list = [f for f in os.listdir(spectrogram_dir) if f.endswith('_red.npy')]\n",
    "        self.labels = labels  #dictionary mapping base file names to labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #to get the base name of the file (without the color channel)\n",
    "        base_name = self.file_list[idx].replace('_red.npy', '')\n",
    "\n",
    "        #All colour channels separately\n",
    "        red_channel = np.load(os.path.join(self.spectrogram_dir, base_name + '_red.npy'))\n",
    "        green_channel = np.load(os.path.join(self.spectrogram_dir, base_name + '_green.npy'))\n",
    "        blue_channel = np.load(os.path.join(self.spectrogram_dir, base_name + '_blue.npy'))\n",
    "\n",
    "        #Stack the color channels to create RGB image\n",
    "        spectrogram = np.stack((red_channel, green_channel, blue_channel), axis=2)\n",
    "\n",
    "        #Conversion to tensor\n",
    "        spectrogram = torch.from_numpy(spectrogram)\n",
    "\n",
    "        #Normalisation to range [-1, 1]\n",
    "        spectrogram = (spectrogram - 0.5) * 2\n",
    "\n",
    "        #Label\n",
    "        label = self.labels[base_name]\n",
    "\n",
    "        return spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "#0.001\n",
    "N_EPOCHS = 10\n",
    "#5\n",
    "batchsize = 64\n",
    "#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load labels for sarcasm detection (from a different file, not part of the spectrograms folder)\n",
    "with open('data/mixed_data.json') as f:\n",
    "    text_data = json.load(f)\n",
    "    sarcasm_labels = {k: int(v['sarcasm']) for k, v in text_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation of the dataset\n",
    "dataset = SpectrogramDataset('data/spectrograms/', sarcasm_labels)\n",
    "\n",
    "# Initialize a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "#Splitting the dataset into training and testing data\n",
    "\n",
    "train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
    "test_size = len(dataset) - train_size  # 20% of the data for testing\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "#DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batchsize)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.7246\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.5593\n",
      "Validation acc: 0.7165\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.5972\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.6156\n",
      "Validation acc: 0.7087\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.5609\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.6337\n",
      "Validation acc: 0.7087\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.5289\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.6517\n",
      "Validation acc: 0.7087\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.5185\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.6656\n",
      "Validation acc: 0.7102\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.5037\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.6769\n",
      "Validation acc: 0.7113\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.4914\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.6858\n",
      "Validation acc: 0.7109\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.4850\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.6927\n",
      "Validation acc: 0.7096\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.4767\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.6992\n",
      "Validation acc: 0.7078\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.4683\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.7045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7063\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load pre-trained MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)  #Two classes for classification\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = SparseCategoricalCrossentropy()\n",
    "\n",
    "# Define the metrics\n",
    "train_acc_metric = SparseCategoricalAccuracy()\n",
    "val_acc_metric = SparseCategoricalAccuracy()\n",
    "\n",
    "# Define the training function\n",
    "def train_cycle(model, optimizer, loss_fn, train_acc_metric, val_acc_metric, train_dataloader, val_dataloader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataloader):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "            # Log every 200 batches.\n",
    "            if step % 200 == 0:\n",
    "                print(\n",
    "                    \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                    % (step, float(loss_value))\n",
    "                )\n",
    "                print(\"Seen so far: %s samples\" % ((step + 1) * 64))\n",
    "\n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for x_batch_val, y_batch_val in val_dataloader:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        val_acc = val_acc_metric.result()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "\n",
    "# Train the mode\n",
    "train_cycle(model, optimizer, loss_fn, train_acc_metric, val_acc_metric, train_dataloader, test_dataloader, epochs=N_EPOCHS)\n",
    "\n",
    "# Save the model weights\n",
    "model.save('models_weights/beit_model_spectrograms.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLPROJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
