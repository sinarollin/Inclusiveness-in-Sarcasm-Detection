{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Data Preparation for the Spectrograms, Data Balancing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import librosa as librosa\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/mixed_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ids = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "633"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keeping the male and female utterances (633):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = 'data/audio/mixed/'\n",
    "keys = ids\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "for filename in files:\n",
    "    #Check if the file is a .wav file\n",
    "    if filename.endswith('.wav'):\n",
    "        #ID from filename\n",
    "        id = filename.split('.')[0]\n",
    "        \n",
    "        #Check if the id is not in the keys\n",
    "        if id not in keys:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 633 .wav files in the audio directory.\n"
     ]
    }
   ],
   "source": [
    "directory = 'data/audio/mixed/'\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "num_wav_files = len(wav_files)\n",
    "\n",
    "print(f'There are {num_wav_files} .wav files in the audio directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding female utterances to a separate folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the JSON file\n",
    "with open('data/F_data.json', 'r') as file:\n",
    "    # Load the contents of the file into a Python object\n",
    "    dataF = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 203 female utterances.\n"
     ]
    }
   ],
   "source": [
    "keys_F = dataF.keys()\n",
    "print(\"There are\" , len(keys_F), \"female utterances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source and target\n",
    "source_dir = 'audio_files/'\n",
    "target_dir = 'data/audio/F'\n",
    "\n",
    "\n",
    "keys = keys_F\n",
    "\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        # Extract the id from the filename\n",
    "        id = filename.split('.')[0]\n",
    "        \n",
    "        if id in keys:\n",
    "            source_file = os.path.join(source_dir, filename)\n",
    "            target_file = os.path.join(target_dir, filename)\n",
    "            \n",
    "            shutil.copy(source_file, target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 203 .wav files in the female utterance audio directory.\n"
     ]
    }
   ],
   "source": [
    "directory = 'data/audio/F'\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Count the number of .wav files\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "num_wav_files = len(wav_files)\n",
    "\n",
    "print(f'There are {num_wav_files} .wav files in the female utterance audio directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same procedure for the male utterances in a separate folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the JSON file\n",
    "with open('data/M_data.json', 'r') as file:\n",
    "    # Load the contents of the file into a Python object\n",
    "    dataM = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_M = dataM.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source and target\n",
    "source_dir = 'audio_files/'\n",
    "target_dir = 'data/audio/M'\n",
    "\n",
    "\n",
    "keys = keys_M\n",
    "\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        # Extract the id from the filename\n",
    "        id = filename.split('.')[0]\n",
    "        \n",
    "        if id in keys:\n",
    "            source_file = os.path.join(source_dir, filename)\n",
    "            target_file = os.path.join(target_dir, filename)\n",
    "            \n",
    "            shutil.copy(source_file, target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 430 .wav files in the male utterance audio directory.\n"
     ]
    }
   ],
   "source": [
    "directory = 'data/audio/M'\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Count the number of .wav files\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "num_wav_files = len(wav_files)\n",
    "\n",
    "print(f'There are {num_wav_files} .wav files in the male utterance audio directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Augmentation and Balancing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As known from the text augmentation and balancing seen in the text_data_preparation file, the sarcasm of the mixed dataset is balanced. Furthermore, it is also known that the male dataset lacks 18 non-sarcastic utterances while the mixed set lacks female utterances in quantity. The female set only has a difference of one between sarcastic and non sarcastic and is therefore considered balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load labels for sarcasm detection (from a different file, not part of the　audio folder)\n",
    "with open('data/M_data.json') as f:\n",
    "    text_data = json.load(f)\n",
    "    sarcasm_labels_M = {k: int(v['sarcasm']) for k, v in text_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sarcasm_labels_M) == 430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUGMENTATION MALE UTTERANCES\n",
    "\n",
    "directory = 'data/audio/M/'\n",
    "label = 'A'  #label for augmented data\n",
    "labels = sarcasm_labels_M\n",
    "\n",
    "\n",
    "\n",
    "files = [file for file in os.listdir(directory) if file.endswith('.wav')]\n",
    "\n",
    "non_sarcastic_files = [file for file in files if labels[file.split('.')[0]] == 0]\n",
    "\n",
    "# Randomly select 18 files\n",
    "selected_files = random.sample(non_sarcastic_files, 18)\n",
    "\n",
    "all_labels = copy.deepcopy(sarcasm_labels_M)\n",
    "\n",
    "for filename in selected_files:\n",
    "    y, sr = librosa.load(os.path.join(directory, filename))\n",
    "    \n",
    "    #Pitch shifting\n",
    "    y_shifted = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=-2)  # Shift pitch down by 2 half-steps\n",
    "    \n",
    "    #New filenames\n",
    "    new_filename = f'{label}_{filename}'\n",
    "    sf.write(os.path.join(directory, new_filename), y_shifted, sr)\n",
    "    \n",
    "    #label for the augmented audio\n",
    "    all_labels[new_filename.split('.')[0]] = 0  # 0 for non-sarcastic\n",
    "\n",
    "# Save all the labels as a JSON file\n",
    "with open('data/audio/labels_M.json', 'w') as f:\n",
    "    json.dump(all_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save these labels locally for the training later, this is how it can be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/audio/labels_M.json', 'r') as f:\n",
    "    labelsM = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(labelsM) == 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is balanced.\n",
      "{0: 224, 1: 224}\n"
     ]
    }
   ],
   "source": [
    "#Checking that the dataset is balanced\n",
    "\n",
    "label_counts = {label: list(labelsM.values()).count(label) for label in set(labelsM.values())}\n",
    "\n",
    "if len(set(label_counts.values())) == 1:\n",
    "    print(\"The dataset is balanced.\")\n",
    "else:\n",
    "    print(\"The dataset is not balanced.\")\n",
    "\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar procedure for the mixed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load labels for sarcasm detection (from a different file, not part of the　audio folder)\n",
    "with open('data/mixed_data.json') as f:\n",
    "    text_data = json.load(f)\n",
    "    sarcasm_labels_mixed = {k: int(v['sarcasm']) for k, v in text_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sarcasm_labels_mixed) == 633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load labels for sarcasm detection (from a different file, not part of the　audio folder)\n",
    "with open('data/F_data.json') as f:\n",
    "    text_data = json.load(f)\n",
    "    sarcasm_labels_F = {k: int(v['sarcasm']) for k, v in text_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sarcasm_labels_F) == 203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sarcasm_labels_F to a JSON file\n",
    "with open('data/audio/labels_F.json', 'w') as f:\n",
    "    json.dump(sarcasm_labels_F, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTROL for correct storing\n",
    "with open('data/audio/labels_F.json', 'r') as f:\n",
    "    labels_F = json.load(f)\n",
    "\n",
    "num_labels = len(labels_F)\n",
    "\n",
    "assert num_labels == 203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 633 .wav files in the mixed utterance audio directory.\n"
     ]
    }
   ],
   "source": [
    "directory = 'data/audio/mixed'\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Count the number of .wav files\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "num_wav_files = len(wav_files)\n",
    "\n",
    "print(f'There are {num_wav_files} .wav files in the mixed utterance audio directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUGMENTATION MIXED UTTERANCES\n",
    "#Augmenting female utterances to match\n",
    "#(1)\n",
    "\n",
    "# Set the directory and label for augmented data\n",
    "directory = 'data/audio/mixed/'\n",
    "label = 'A1'\n",
    "\n",
    "# Use the female labels\n",
    "labels = sarcasm_labels_F\n",
    "\n",
    "# Get all the .wav files in the directory\n",
    "files = [file for file in os.listdir(directory) if file.endswith('.wav')]\n",
    "\n",
    "# Separate the sarcastic and non-sarcastic files\n",
    "sarcastic_files = [file for file in files if file.split('.')[0] in labels and labels[file.split('.')[0]] == 1]\n",
    "non_sarcastic_files = [file for file in files if file.split('.')[0] in labels and labels[file.split('.')[0]] == 0]\n",
    "\n",
    "# Randomly select files for augmentation\n",
    "selected_sarcastic_files = random.sample(sarcastic_files, 52) \n",
    "selected_non_sarcastic_files = random.sample(non_sarcastic_files, 62)\n",
    "\n",
    "# Copy the original labels\n",
    "all_labels = copy.deepcopy(sarcasm_labels_mixed)\n",
    "\n",
    "# Perform the first augmentation on the selected sarcastic files\n",
    "for filename in selected_sarcastic_files:\n",
    "    y, sr = librosa.load(os.path.join(directory, filename))\n",
    "    y_shifted = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=-2)  # Shift pitch down by 2 half-steps\n",
    "    new_filename = f'{label}_{filename}'\n",
    "    sf.write(os.path.join(directory, new_filename), y_shifted, sr)\n",
    "    all_labels[new_filename.split('.')[0]] = 1  # 1 for sarcastic\n",
    "\n",
    "# Perform the second augmentation on the selected non-sarcastic files\n",
    "# Modify the augmentation as needed\n",
    "for filename in selected_non_sarcastic_files:\n",
    "    y, sr = librosa.load(os.path.join(directory, filename))\n",
    "    y_shifted = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=2)  # Shift pitch down by 2 half-steps\n",
    "    new_filename = f'{label}_{filename}'\n",
    "    sf.write(os.path.join(directory, new_filename), y_shifted, sr)\n",
    "    all_labels[new_filename.split('.')[0]] = 0  # 0 for non-sarcastic\n",
    "\n",
    "# Save all the labels as a JSON file\n",
    "with open('data/audio/labels_mixed.json', 'w') as f:\n",
    "    json.dump(all_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 747 .wav files in the mixed utterance audio directory after the first augmentation.\n"
     ]
    }
   ],
   "source": [
    "directory = 'data/audio/mixed'\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Count the number of .wav files\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "num_wav_files = len(wav_files)\n",
    "\n",
    "print(f'There are now {num_wav_files} .wav files in the mixed utterance audio directory after the first augmentation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do it again for the second augmentation\n",
    "#(2)\n",
    "#AUGMENTATION MIXED UTTERANCES\n",
    "#Augmenting female utterances to match\n",
    "#(1)\n",
    "\n",
    "# Set the directory and label for augmented data\n",
    "directory = 'data/audio/mixed/'\n",
    "label = 'A2'\n",
    "\n",
    "# Use the female labels\n",
    "labels = sarcasm_labels_F\n",
    "\n",
    "# Get all the .wav files in the directory\n",
    "files = [file for file in os.listdir(directory) if file.endswith('.wav')]\n",
    "\n",
    "# Separate the sarcastic and non-sarcastic files\n",
    "sarcastic_files = [file for file in files if file.split('.')[0] in labels and labels[file.split('.')[0]] == 1]\n",
    "non_sarcastic_files = [file for file in files if file.split('.')[0] in labels and labels[file.split('.')[0]] == 0]\n",
    "\n",
    "# Randomly select files for augmentation\n",
    "selected_sarcastic_files = random.sample(sarcastic_files, 52) \n",
    "selected_non_sarcastic_files = random.sample(non_sarcastic_files, 61)\n",
    "\n",
    "# Load the existing labels\n",
    "with open('data/audio/labels_mixed.json', 'r') as f:\n",
    "    all_labels = json.load(f)\n",
    "\n",
    "# Perform the first augmentation on the selected sarcastic files\n",
    "for filename in selected_sarcastic_files:\n",
    "    y, sr = librosa.load(os.path.join(directory, filename))\n",
    "    noise = np.random.normal(0, 0.007, y.shape)  # Generate Gaussian noise\n",
    "    y_noisy = y + noise  # Add noise to the original audio\n",
    "    new_filename = f'{label}_{filename}'\n",
    "    sf.write(os.path.join(directory, new_filename), y_noisy, sr)\n",
    "    all_labels[new_filename.split('.')[0]] = 1  # 1 for sarcastic\n",
    "\n",
    "# Perform the second augmentation on the selected non-sarcastic files\n",
    "for filename in selected_non_sarcastic_files:\n",
    "    y, sr = librosa.load(os.path.join(directory, filename))\n",
    "    noise = np.random.normal(0, 0.007, y.shape)  # Generate Gaussian noise\n",
    "    y_noisy = y + noise  # Add noise to the original audio\n",
    "    new_filename = f'{label}_{filename}'\n",
    "    sf.write(os.path.join(directory, new_filename), y_noisy, sr)\n",
    "    all_labels[new_filename.split('.')[0]] = 0  # 0 for non-sarcastic\n",
    "\n",
    "\n",
    "# Save all the labels as a JSON file\n",
    "with open('data/audio/labels_mixed.json', 'w') as f:\n",
    "    json.dump(all_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 860 .wav files in the mixed utterance audio directory after the second augmentation.\n"
     ]
    }
   ],
   "source": [
    "directory = 'data/audio/mixed'\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Count the number of .wav files\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "num_wav_files = len(wav_files)\n",
    "\n",
    "print(f'There are now {num_wav_files} .wav files in the mixed utterance audio directory after the second augmentation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that all the correct audio files are present we are going to rely on the labels that were created at the same to check that the sets are now balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic count: 430\n",
      "Non-sarcastic count: 430\n",
      "Male count in mixed labels: 430\n",
      "Female count in mixed labels: 203\n",
      "Sarcastic count in  female labels: 102\n",
      "Non-sarcastic count in female labels: 101\n",
      "Sarcastic count in sarcasmM_labels: 224\n",
      "Non-sarcastic count in sarcasmM_labels: 224\n"
     ]
    }
   ],
   "source": [
    "# Load the labels\n",
    "with open('data/audio/labels_mixed.json', 'r') as f:\n",
    "    mixed_labels = json.load(f)\n",
    "\n",
    "with open('data/audio/labels_F.json', 'r') as f:\n",
    "    labels_F = json.load(f)\n",
    "\n",
    "with open('data/audio/labels_M.json', 'r') as f:\n",
    "    labels_M = json.load(f)\n",
    "\n",
    "# Mixed\n",
    "sarcastic_count = sum(1 for label in mixed_labels.values() if label == 1)\n",
    "non_sarcastic_count = sum(1 for label in mixed_labels.values() if label == 0)\n",
    "\n",
    "# Mixed: F vs M\n",
    "male_count_mixed = sum(1 for id in mixed_labels.keys() if any(id == label for label in labels_M.keys()))\n",
    "female_count_mixed = sum(1 for id in mixed_labels.keys() if any(id == label for label in labels_F.keys()))\n",
    "\n",
    "# Female Set\n",
    "sarcastic_count_F = sum(1 for label in labels_F.values() if label == 1)\n",
    "non_sarcastic_count_F = sum(1 for label in labels_F.values() if label == 0)\n",
    "# Male Set\n",
    "sarcastic_count_M = sum(1 for label in labels_M.values() if label == 1)\n",
    "non_sarcastic_count_M = sum(1 for label in labels_M.values() if label == 0)\n",
    "\n",
    "print(f'Sarcastic count: {sarcastic_count}')\n",
    "print(f'Non-sarcastic count: {non_sarcastic_count}')\n",
    "print(f'Male count in mixed labels: {male_count_mixed}')\n",
    "print(f'Female count in mixed labels: {female_count_mixed}')\n",
    "print(f'Sarcastic count in  female labels: {sarcastic_count_F}')\n",
    "print(f'Non-sarcastic count in female labels: {non_sarcastic_count_F}')\n",
    "print(f'Sarcastic count in sarcasmM_labels: {sarcastic_count_M}')\n",
    "print(f'Non-sarcastic count in sarcasmM_labels: {non_sarcastic_count_M}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now to create the correct spectrograms for each of the three datasets:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT TO BE RE-RUN\n",
    "#Creation of spectrograms from the waveform data sorted into their proper directories\n",
    "\n",
    "#FEMALE UTTERANCES\n",
    "\n",
    "#Directory containing audio files\n",
    "audio_dir = 'data/audio/F/'\n",
    "\n",
    "#Directory to save spectrograms\n",
    "spectrogram_dir = 'data/audio/spectrograms_F/'\n",
    "\n",
    "#Length of the maximum waveform found in the dataset\n",
    "# 1_213.wav\n",
    "max_length = 882882\n",
    "\n",
    "\n",
    "for file_name in os.listdir(audio_dir):\n",
    "    if file_name.endswith('.wav'):\n",
    "        file_path = os.path.join(audio_dir, file_name)\n",
    "        y, sr = librosa.load(file_path)\n",
    "\n",
    "        #Padding the waveform to the maximum length found\n",
    "        if len(y) < max_length:\n",
    "            y_padded = np.pad(y, (0, max_length - len(y)), 'constant')\n",
    "        else:\n",
    "            y_padded = y[:max_length]\n",
    "\n",
    "        #Compute the short-time Fourier transform\n",
    "        D = librosa.stft(y_padded)\n",
    "\n",
    "        #Convert the amplitude to decibels (logarithmic scale)\n",
    "        D_log = librosa.amplitude_to_db(abs(D))\n",
    "\n",
    "        #Resize the spectrogram to 224x224 -> wanted dimensions for the Beit model\n",
    "        resize_factor_x = 224 / D_log.shape[1]\n",
    "        resize_factor_y = 224 / D_log.shape[0]\n",
    "        D_log_resized = zoom(D_log, (resize_factor_y, resize_factor_x))\n",
    "\n",
    "        #Conversion to colour image\n",
    "        D_log_resized_color = plt.get_cmap('viridis')(D_log_resized / np.amax(D_log_resized))\n",
    "\n",
    "        #Remove the alpha channel of the RGBA image\n",
    "        D_log_resized_color = D_log_resized_color[:, :, :3]\n",
    "\n",
    "        #All colour channels saved in the same variable\n",
    "        red_channel = D_log_resized_color[:, :, 0]\n",
    "        green_channel = D_log_resized_color[:, :, 1]\n",
    "        blue_channel = D_log_resized_color[:, :, 2]\n",
    "\n",
    "        #Save the spectrograms separately for each colour channel\n",
    "        red_spectrogram_path = os.path.join(spectrogram_dir, file_name.replace('.wav', '_red.npy'))\n",
    "        green_spectrogram_path = os.path.join(spectrogram_dir, file_name.replace('.wav', '_green.npy'))\n",
    "        blue_spectrogram_path = os.path.join(spectrogram_dir, file_name.replace('.wav', '_blue.npy'))\n",
    "\n",
    "        np.save(red_spectrogram_path, red_channel)\n",
    "        np.save(green_spectrogram_path, green_channel)\n",
    "        np.save(blue_spectrogram_path, blue_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check whether correct number of files\n",
    "directory = 'data/audio/spectrograms_F/' \n",
    "\n",
    "files = os.listdir(directory)\n",
    "num_files = len(files)\n",
    "\n",
    "assert num_files == 3*len(os.listdir('data/audio/F/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT TO BE RE-RUN\n",
    "#Creation of spectrograms from the waveform data sorted into their proper directories\n",
    "\n",
    "#MALE UTTERANCES\n",
    "\n",
    "#Directory containing audio files\n",
    "audio_dir = 'data/audio/M/'\n",
    "\n",
    "#Directory to save spectrograms\n",
    "spectrogram_dir = 'data/audio/spectrograms_M/'\n",
    "\n",
    "#Length of the maximum waveform found in the dataset\n",
    "# 1_213.wav\n",
    "max_length = 882882\n",
    "\n",
    "\n",
    "for file_name in os.listdir(audio_dir):\n",
    "    if file_name.endswith('.wav'):\n",
    "        file_path = os.path.join(audio_dir, file_name)\n",
    "        y, sr = librosa.load(file_path)\n",
    "\n",
    "        #Padding the waveform to the maximum length found\n",
    "        if len(y) < max_length:\n",
    "            y_padded = np.pad(y, (0, max_length - len(y)), 'constant')\n",
    "        else:\n",
    "            y_padded = y[:max_length]\n",
    "\n",
    "        #Compute the short-time Fourier transform\n",
    "        D = librosa.stft(y_padded)\n",
    "\n",
    "        #Convert the amplitude to decibels (logarithmic scale)\n",
    "        D_log = librosa.amplitude_to_db(abs(D))\n",
    "\n",
    "        #Resize the spectrogram to 224x224 -> wanted dimensions for the Beit model\n",
    "        resize_factor_x = 224 / D_log.shape[1]\n",
    "        resize_factor_y = 224 / D_log.shape[0]\n",
    "        D_log_resized = zoom(D_log, (resize_factor_y, resize_factor_x))\n",
    "\n",
    "        #Conversion to colour image\n",
    "        D_log_resized_color = plt.get_cmap('viridis')(D_log_resized / np.amax(D_log_resized))\n",
    "\n",
    "        #Remove the alpha channel of the RGBA image\n",
    "        D_log_resized_color = D_log_resized_color[:, :, :3]\n",
    "\n",
    "        #All colour channels saved in the same variable\n",
    "        red_channel = D_log_resized_color[:, :, 0]\n",
    "        green_channel = D_log_resized_color[:, :, 1]\n",
    "        blue_channel = D_log_resized_color[:, :, 2]\n",
    "\n",
    "        #Save the spectrograms separately for each colour channel\n",
    "        red_spectrogram_path = os.path.join(spectrogram_dir, file_name.replace('.wav', '_red.npy'))\n",
    "        green_spectrogram_path = os.path.join(spectrogram_dir, file_name.replace('.wav', '_green.npy'))\n",
    "        blue_spectrogram_path = os.path.join(spectrogram_dir, file_name.replace('.wav', '_blue.npy'))\n",
    "\n",
    "        np.save(red_spectrogram_path, red_channel)\n",
    "        np.save(green_spectrogram_path, green_channel)\n",
    "        np.save(blue_spectrogram_path, blue_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check whether correct number of files\n",
    "directory = 'data/audio/spectrograms_M/' \n",
    "\n",
    "files = os.listdir(directory)\n",
    "num_files = len(files)\n",
    "\n",
    "assert num_files == 3*len(os.listdir('data/audio/M/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT TO BE RE-RUN\n",
    "#Creation of spectrograms from the waveform data sorted into their proper directories\n",
    "\n",
    "#MIXED UTTERANCES\n",
    "\n",
    "#Directory containing audio files\n",
    "audio_dir = 'data/audio/mixed/'\n",
    "\n",
    "#Directory to save spectrograms\n",
    "spectrogram_dir = 'data/audio/spectrograms_mixed/'\n",
    "\n",
    "#Length of the maximum waveform found in the dataset\n",
    "# 1_213.wav\n",
    "max_length = 882882\n",
    "\n",
    "\n",
    "for file_name in os.listdir(audio_dir):\n",
    "    if file_name.endswith('.wav'):\n",
    "        file_path = os.path.join(audio_dir, file_name)\n",
    "        y, sr = librosa.load(file_path)\n",
    "\n",
    "        #Padding the waveform to the maximum length found\n",
    "        if len(y) < max_length:\n",
    "            y_padded = np.pad(y, (0, max_length - len(y)), 'constant')\n",
    "        else:\n",
    "            y_padded = y[:max_length]\n",
    "\n",
    "        #Compute the short-time Fourier transform\n",
    "        D = librosa.stft(y_padded)\n",
    "\n",
    "        #Convert the amplitude to decibels (logarithmic scale)\n",
    "        D_log = librosa.amplitude_to_db(abs(D))\n",
    "\n",
    "        #Resize the spectrogram to 224x224 -> wanted dimensions for the Beit model\n",
    "        resize_factor_x = 224 / D_log.shape[1]\n",
    "        resize_factor_y = 224 / D_log.shape[0]\n",
    "        D_log_resized = zoom(D_log, (resize_factor_y, resize_factor_x))\n",
    "\n",
    "        #Conversion to colour image\n",
    "        D_log_resized_color = plt.get_cmap('viridis')(D_log_resized / np.amax(D_log_resized))\n",
    "\n",
    "        #Remove the alpha channel of the RGBA image\n",
    "        D_log_resized_color = D_log_resized_color[:, :, :3]\n",
    "\n",
    "        #All colour channels saved in the same variable\n",
    "        red_channel = D_log_resized_color[:, :, 0]\n",
    "        green_channel = D_log_resized_color[:, :, 1]\n",
    "        blue_channel = D_log_resized_color[:, :, 2]\n",
    "\n",
    "        #Save the spectrograms separately for each colour channel\n",
    "        red_spectrogram_path = os.path.join(spectrogram_dir, file_name.replace('.wav', '_red.npy'))\n",
    "        green_spectrogram_path = os.path.join(spectrogram_dir, file_name.replace('.wav', '_green.npy'))\n",
    "        blue_spectrogram_path = os.path.join(spectrogram_dir, file_name.replace('.wav', '_blue.npy'))\n",
    "\n",
    "        np.save(red_spectrogram_path, red_channel)\n",
    "        np.save(green_spectrogram_path, green_channel)\n",
    "        np.save(blue_spectrogram_path, blue_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check whether correct number of files\n",
    "directory = 'data/audio/spectrograms_mixed/' \n",
    "\n",
    "files = os.listdir(directory)\n",
    "num_files = len(files)\n",
    "\n",
    "assert num_files == 3*len(os.listdir('data/audio/mixed/'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLPROJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
