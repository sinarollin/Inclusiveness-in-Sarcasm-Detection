{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authors: Céline Hirsch, Sandra Frey, Sina Röllin\n",
    "#Deep Learning Project: Inclusiveness in Sarcasm Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_audio_model_tiny import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some classes and functions needed in the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, spectrogram_dir, labels):\n",
    "        self.spectrogram_dir = spectrogram_dir\n",
    "        self.file_list = [f for f in os.listdir(spectrogram_dir) if f.endswith('_red.npy')]\n",
    "        self.labels = labels  #dictionary mapping base file names to labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #to get the base name of the file (without the color channel)\n",
    "        base_name = self.file_list[idx].replace('_red.npy', '')\n",
    "\n",
    "        #All colour channels separately\n",
    "        red_channel = np.load(os.path.join(self.spectrogram_dir, base_name + '_red.npy'))\n",
    "        green_channel = np.load(os.path.join(self.spectrogram_dir, base_name + '_green.npy'))\n",
    "        blue_channel = np.load(os.path.join(self.spectrogram_dir, base_name + '_blue.npy'))\n",
    "\n",
    "        #Stack the color channels to create RGB image\n",
    "        spectrogram = np.stack((red_channel, green_channel, blue_channel), axis=2)\n",
    "\n",
    "        #Conversion to tensor\n",
    "        spectrogram = torch.from_numpy(spectrogram)\n",
    "\n",
    "        #Normalisation to range [-1, 1]\n",
    "        spectrogram = (spectrogram - 0.5) * 2\n",
    "\n",
    "        #Label\n",
    "        label = self.labels[base_name]\n",
    "\n",
    "        return spectrogram, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to load the sarcasm labels. We can extract these from the datasets created for the text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels for sarcasm detection (from a different file, not part of the spectrograms folder)\n",
    "# Load the labels\n",
    "with open('data/audio/labels_mixed.json', 'r') as f:\n",
    "    mixed_labels = json.load(f)\n",
    "\n",
    "with open('data/audio/labels_F.json', 'r') as f:\n",
    "    F_labels = json.load(f)\n",
    "\n",
    "with open('data/audio/labels_M.json', 'r') as f:\n",
    "    M_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize the Datasets, by combining the sarcasm labels with the audio data in the form of spectrograms, as prepared in `audio-data-preparation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the datasets\n",
    "mixed_dataset = SpectrogramDataset('data/spectrograms/spectrograms_mixed/', mixed_labels)\n",
    "M_dataset = SpectrogramDataset('data/spectrograms/spectrograms_M/', M_labels)\n",
    "F_dataset = SpectrogramDataset('data/spectrograms/spectrograms_F/', F_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train 3 different models: \n",
    "- one model will be trained on the mixed data\n",
    "- one model will be trained on spectrograms from female speakers only\n",
    "- one model will be trained on spectrograms from male speakers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training acc over epoch: 0.5029\n",
      "Training F1 score over epoch: 0.3893\n",
      "Training loss over epoch: 0.7462\n",
      "Validation acc: 0.5349\n",
      "Validation F1 score: 0.4631\n",
      "Validation loss: 0.6913\n",
      "\n",
      "Start of epoch 1\n",
      "Training acc over epoch: 0.5749\n",
      "Training F1 score over epoch: 0.5622\n",
      "Training loss over epoch: 0.6479\n",
      "Validation acc: 0.5669\n",
      "Validation F1 score: 0.5626\n",
      "Validation loss: 0.6672\n",
      "\n",
      "Start of epoch 2\n",
      "Training acc over epoch: 0.6143\n",
      "Training F1 score over epoch: 0.5935\n",
      "Training loss over epoch: 0.6055\n",
      "Validation acc: 0.5891\n",
      "Validation F1 score: 0.5971\n",
      "Validation loss: 0.6594\n",
      "\n",
      "Start of epoch 3\n",
      "Training acc over epoch: 0.6384\n",
      "Training F1 score over epoch: 0.6203\n",
      "Training loss over epoch: 0.5850\n",
      "Validation acc: 0.6017\n",
      "Validation F1 score: 0.6215\n",
      "Validation loss: 0.6629\n",
      "\n",
      "Start of epoch 4\n",
      "Training acc over epoch: 0.6526\n",
      "Training F1 score over epoch: 0.6350\n",
      "Training loss over epoch: 0.5744\n",
      "Validation acc: 0.6105\n",
      "Validation F1 score: 0.6355\n",
      "Validation loss: 0.6657\n",
      "\n",
      "Start of epoch 5\n",
      "Training acc over epoch: 0.6621\n",
      "Training F1 score over epoch: 0.6448\n",
      "Training loss over epoch: 0.5673\n",
      "Validation acc: 0.6153\n",
      "Validation F1 score: 0.6447\n",
      "Validation loss: 0.6671\n",
      "\n",
      "Start of epoch 6\n",
      "Training acc over epoch: 0.6701\n",
      "Training F1 score over epoch: 0.6524\n",
      "Training loss over epoch: 0.5616\n",
      "Validation acc: 0.6196\n",
      "Validation F1 score: 0.6524\n",
      "Validation loss: 0.6671\n",
      "\n",
      "Start of epoch 7\n",
      "Training acc over epoch: 0.6764\n",
      "Training F1 score over epoch: 0.6585\n",
      "Training loss over epoch: 0.5568\n",
      "Validation acc: 0.6228\n",
      "Validation F1 score: 0.6582\n",
      "Validation loss: 0.6661\n",
      "\n",
      "Start of epoch 8\n",
      "Training acc over epoch: 0.6814\n",
      "Training F1 score over epoch: 0.6631\n",
      "Training loss over epoch: 0.5526\n",
      "Validation acc: 0.6247\n",
      "Validation F1 score: 0.6625\n",
      "Validation loss: 0.6646\n",
      "\n",
      "Start of epoch 9\n",
      "Training acc over epoch: 0.6856\n",
      "Training F1 score over epoch: 0.6667\n",
      "Training loss over epoch: 0.5490\n",
      "Validation acc: 0.6262\n",
      "Validation F1 score: 0.6662\n",
      "Validation loss: 0.6629\n",
      "\n",
      "Start of epoch 10\n",
      "Training acc over epoch: 0.6893\n",
      "Training F1 score over epoch: 0.6699\n",
      "Training loss over epoch: 0.5457\n",
      "Validation acc: 0.6274\n",
      "Validation F1 score: 0.6693\n",
      "Validation loss: 0.6611\n",
      "\n",
      "Start of epoch 11\n",
      "Training acc over epoch: 0.6925\n",
      "Training F1 score over epoch: 0.6725\n",
      "Training loss over epoch: 0.5425\n",
      "Validation acc: 0.6284\n",
      "Validation F1 score: 0.6720\n",
      "Validation loss: 0.6592\n",
      "\n",
      "Start of epoch 12\n",
      "Training acc over epoch: 0.6951\n",
      "Training F1 score over epoch: 0.6747\n",
      "Training loss over epoch: 0.5396\n",
      "Validation acc: 0.6288\n",
      "Validation F1 score: 0.6741\n",
      "Validation loss: 0.6574\n",
      "\n",
      "Start of epoch 13\n",
      "Training acc over epoch: 0.6974\n",
      "Training F1 score over epoch: 0.6765\n",
      "Training loss over epoch: 0.5368\n",
      "Validation acc: 0.6292\n",
      "Validation F1 score: 0.6759\n",
      "Validation loss: 0.6556\n",
      "\n",
      "Start of epoch 14\n",
      "Training acc over epoch: 0.6993\n",
      "Training F1 score over epoch: 0.6780\n",
      "Training loss over epoch: 0.5342\n",
      "Validation acc: 0.6306\n",
      "Validation F1 score: 0.6777\n",
      "Validation loss: 0.6539\n",
      "\n",
      "Start of epoch 15\n",
      "Training acc over epoch: 0.7011\n",
      "Training F1 score over epoch: 0.6797\n",
      "Training loss over epoch: 0.5318\n",
      "Validation acc: 0.6319\n",
      "Validation F1 score: 0.6794\n",
      "Validation loss: 0.6523\n",
      "\n",
      "Start of epoch 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m train_acc_metric \u001b[38;5;241m=\u001b[39m SparseCategoricalAccuracy()\n\u001b[0;32m     46\u001b[0m val_acc_metric \u001b[38;5;241m=\u001b[39m SparseCategoricalAccuracy()\n\u001b[1;32m---> 48\u001b[0m train_accs, val_accs, train_losses, val_losses, train_f1s, val_f1s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_acc_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_acc_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m plot_metrics(train_accs, val_accs, train_losses, val_losses, train_f1s, val_f1s)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Save the model weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\celin\\OneDrive\\Documents\\EPFL\\MA2\\Deep learning\\Inclusivity-in-Sarcasm-Detection\\functions_audio_model_tiny.py:24\u001b[0m, in \u001b[0;36mtrain_cycle\u001b[1;34m(model, optimizer, loss_fn, train_acc_metric, val_acc_metric, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[0;32m     21\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Iterate over the batches of the dataset.\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGradientTape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtape\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\celin\\anaconda3\\envs\\DLproj\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\celin\\anaconda3\\envs\\DLproj\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\celin\\anaconda3\\envs\\DLproj\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\celin\\anaconda3\\envs\\DLproj\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\celin\\anaconda3\\envs\\DLproj\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mSpectrogramDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m blue_channel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrogram_dir, base_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_blue.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Stack the color channels to create RGB image\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mred_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreen_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblue_channel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#Conversion to tensor\u001b[39;00m\n\u001b[0;32m     23\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(spectrogram)\n",
      "File \u001b[1;32mc:\\Users\\celin\\anaconda3\\envs\\DLproj\\Lib\\site-packages\\numpy\\core\\shape_base.py:456\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    454\u001b[0m sl \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) \u001b[38;5;241m*\u001b[39m axis \u001b[38;5;241m+\u001b[39m (_nx\u001b[38;5;241m.\u001b[39mnewaxis,)\n\u001b[0;32m    455\u001b[0m expanded_arrays \u001b[38;5;241m=\u001b[39m [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MIXED MODEL\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 29\n",
    "batch_size = 32\n",
    "weight_decay = 0.1\n",
    "dropout_prob = 0.2\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(0.8 * len(mixed_dataset))  # 80% of the data for training\n",
    "test_size = len(mixed_dataset) - train_size  # 20% of the data for testing\n",
    "train_dataset, test_dataset = random_split(mixed_dataset, [train_size, test_size])\n",
    "\n",
    "# Initialize the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model (pre-trained MobileNetV2)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)  # Two classes for classification\n",
    "\n",
    "mixed_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = SparseCategoricalCrossentropy()\n",
    "\n",
    "# Define the metrics\n",
    "train_acc_metric = SparseCategoricalAccuracy()\n",
    "val_acc_metric = SparseCategoricalAccuracy()\n",
    "\n",
    "train_accs, val_accs, train_losses, val_losses, train_f1s, val_f1s = train_cycle(mixed_model, optimizer, loss_fn, train_acc_metric, val_acc_metric, train_dataloader, test_dataloader, epochs=num_epochs)\n",
    "plot_metrics(train_accs, val_accs, train_losses, val_losses, train_f1s, val_f1s)\n",
    "\n",
    "# Save the model weights\n",
    "mixed_model.save('models/mixed_model_audio.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMALE MODEL\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 29\n",
    "batch_size = 32\n",
    "weight_decay = 0.1\n",
    "dropout_prob = 0.2\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(0.8 * len(F_dataset))  # 80% of the data for training\n",
    "test_size = len(F_dataset) - train_size  # 20% of the data for testing\n",
    "train_dataset, test_dataset = random_split(F_dataset, [train_size, test_size])\n",
    "\n",
    "# Initialize the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model (pre-trained MobileNetV2)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)  # Two classes for classification\n",
    "\n",
    "F_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = SparseCategoricalCrossentropy()\n",
    "\n",
    "# Define the metrics\n",
    "train_acc_metric = SparseCategoricalAccuracy()\n",
    "val_acc_metric = SparseCategoricalAccuracy()\n",
    "\n",
    "train_accs, val_accs, train_losses, val_losses, train_f1s, val_f1s = train_cycle(F_model, optimizer, loss_fn, train_acc_metric, val_acc_metric, train_dataloader, test_dataloader, epochs=num_epochs)\n",
    "plot_metrics(train_accs, val_accs, train_losses, val_losses, train_f1s, val_f1s)\n",
    "\n",
    "# Save the model weights\n",
    "F_model.save('models/F_model_audio.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MALE MODEL\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 29\n",
    "batch_size = 32\n",
    "weight_decay = 0.1\n",
    "dropout_prob = 0.2\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(0.8 * len(M_dataset))  # 80% of the data for training\n",
    "test_size = len(M_dataset) - train_size  # 20% of the data for testing\n",
    "train_dataset, test_dataset = random_split(M_dataset, [train_size, test_size])\n",
    "\n",
    "# Initialize the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model (pre-trained MobileNetV2)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)  # Two classes for classification\n",
    "\n",
    "M_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = SparseCategoricalCrossentropy()\n",
    "\n",
    "# Define the metrics\n",
    "train_acc_metric = SparseCategoricalAccuracy()\n",
    "val_acc_metric = SparseCategoricalAccuracy()\n",
    "\n",
    "train_accs, val_accs, train_losses, val_losses, train_f1s, val_f1s = train_cycle(M_model, optimizer, loss_fn, train_acc_metric, val_acc_metric, train_dataloader, test_dataloader, epochs=num_epochs)\n",
    "plot_metrics(train_accs, val_accs, train_losses, val_losses, train_f1s, val_f1s)\n",
    "\n",
    "# Save the model weights\n",
    "M_model.save('models/M_model_audio.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
