{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the program used to hyperparameter tune our hyperparameters with pytorch:\n",
    "Learning Rate\n",
    "Number of Epochs\n",
    "Batch Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_text_model import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\celin\\OneDrive\\Documents\\EPFL\\MA2\\Deep learning\\Inclusivity-in-Sarcasm-Detection\n",
      "Contents of current directory: ['.git', '.gitignore', 'audio_model-spectrogram.ipynb', 'data', 'functions_audio_model.py', 'functions_text_model.py', 'functions_video_model.py', 'hubert_base_online_tokenizer.ipynb', 'hyperparameter_tuning.ipynb', 'hyperparameter_tuning_text_context.ipynb', 'models', 'README.md', 'requirements.txt', 'results', 'results.json', 'results_hyperparameter_text_context_mixed.json', 'results_tinybert.json', 'results_tinybert_linear_added.json', 'text-data-preparation.ipynb', 'text-model-evaluation.ipynb', 'text-model-training.ipynb', 'text-model-training_context.ipynb', 'Tryouts', 'video-model-training.ipynb', 'vocab.json', 'z-old-text-model-BERT-training.ipynb', 'z-old-text-model.ipynb', '__pycache__']\n",
      "c:\\Users\\celin\\OneDrive\\Documents\\EPFL\\MA2\\Deep learning\\Inclusivity-in-Sarcasm-Detection\\data\\mixed_data.json\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(\"Current directory:\", current_directory)\n",
    "\n",
    "contents = os.listdir(current_directory)\n",
    "print(\"Contents of current directory:\", contents)\n",
    "\n",
    "#### Change the Dataset here when neccessary!\n",
    "json_file_path = os.path.join(current_directory, 'data', 'mixed_data.json')\n",
    "print(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\celin\\anaconda3\\envs\\DLproj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Function to encode the text\n",
    "def encode_text(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Input text\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attention masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "# PyTorch Dataset\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        utterance = item['utterance']\n",
    "        context = item['context']\n",
    "        utterance_and_context = ' '.join([sentence for sentence in context] + [utterance]) # Combining the utterance and its context into one string.\n",
    "        sarcasm = int(item['sarcasm'])\n",
    "        input_ids, attention_mask = encode_text(utterance_and_context)\n",
    "        return input_ids.flatten(), attention_mask.flatten(), sarcasm\n",
    "    \n",
    "# Create the DataLoader\n",
    "# Load the data from the JSON file\n",
    "with open(json_file_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the data to a list of dictionaries\n",
    "data = list(data.values())\n",
    "\n",
    "dataset = SarcasmDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6848,  ACC: 0.5815, F1-weighted: 0.4975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.6881,  ACC: 0.5433, F1-weighted: 0.4684\n",
      "[{'lr': 0.001, 'batch_size': 8, 'num_epochs': 7, 'eval_loss': 0.8265854511409998, 'eval_metrics': {'ACC': 0.6138392857142857, 'F1-weighted': 0.5397935397935398}}, {'lr': 0.0001, 'batch_size': 8, 'num_epochs': 7, 'eval_loss': 0.6539842803031206, 'eval_metrics': {'ACC': 0.7410714285714286, 'F1-weighted': 0.6933032245532246}}, {'lr': 1e-05, 'batch_size': 8, 'num_epochs': 7, 'eval_loss': 0.6643021740019321, 'eval_metrics': {'ACC': 0.6774553571428571, 'F1-weighted': 0.6097031440781442}}, {'lr': 0.001, 'batch_size': 16, 'num_epochs': 7, 'eval_loss': 1.209587313234806, 'eval_metrics': {'ACC': 0.590625, 'F1-weighted': 0.5817139355742297}}, {'lr': 0.0001, 'batch_size': 16, 'num_epochs': 7, 'eval_loss': 0.5911248326301575, 'eval_metrics': {'ACC': 0.678125, 'F1-weighted': 0.6661327438900969}}, {'lr': 1e-05, 'batch_size': 16, 'num_epochs': 7, 'eval_loss': 0.6588964387774467, 'eval_metrics': {'ACC': 0.6864583333333334, 'F1-weighted': 0.6671410278318173}}, {'lr': 0.001, 'batch_size': 32, 'num_epochs': 7, 'eval_loss': 1.2766003906726837, 'eval_metrics': {'ACC': 0.5675403225806451, 'F1-weighted': 0.5230208795692272}}, {'lr': 0.0001, 'batch_size': 32, 'num_epochs': 7, 'eval_loss': 0.5659460499882698, 'eval_metrics': {'ACC': 0.716733870967742, 'F1-weighted': 0.7098741620219734}}, {'lr': 1e-05, 'batch_size': 32, 'num_epochs': 7, 'eval_loss': 0.6873268634080887, 'eval_metrics': {'ACC': 0.5189012096774194, 'F1-weighted': 0.41093910001137784}}, {'lr': 0.001, 'batch_size': 64, 'num_epochs': 7, 'eval_loss': 0.7531097233295441, 'eval_metrics': {'ACC': 0.6535218253968254, 'F1-weighted': 0.652803802972802}}, {'lr': 0.0001, 'batch_size': 64, 'num_epochs': 7, 'eval_loss': 0.5692697465419769, 'eval_metrics': {'ACC': 0.7399553571428572, 'F1-weighted': 0.7367211075768697}}, {'lr': 1e-05, 'batch_size': 64, 'num_epochs': 7, 'eval_loss': 0.688065379858017, 'eval_metrics': {'ACC': 0.5432787698412698, 'F1-weighted': 0.46842967563057225}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "dropout_prob = 0\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 1e-4, 1e-5],\n",
    "    'num_epochs': [7],\n",
    "    'batch_size': [8, 16, 32, 64],\n",
    "    'weight_decay': [0.05],\n",
    "    'dropout_prob': [0]\n",
    "}\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Define the size of the training set and the test set\n",
    "train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
    "test_size = len(dataset) - train_size  # 20% of the data for testing\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "# For each combination of hyperparameters\n",
    "for params in grid:\n",
    "\n",
    "    # Create the DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "    # Create a new model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"prajjwal1/bert-tiny\",\n",
    "        num_labels = 2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(params['dropout_prob']),\n",
    "        nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=16, out_features=2, bias=True)\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Create a new optimizer with the current learning rate\n",
    "    optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    # Create the optimizer  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define metrics\n",
    "    metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "    # Train and evaluate the model for the current number of epochs\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        print('learning rate:', params['lr'], 'batch size:', params['batch_size'], 'num_epochs:', params['num_epochs'])\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_dataloader, device)\n",
    "        eval_loss, eval_metrics = evaluate(model, criterion, metrics, test_dataloader, device)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'lr': params['lr'],\n",
    "        'batch_size': params['batch_size'],\n",
    "        'num_epochs': params['num_epochs'],\n",
    "        'eval_loss': eval_loss,\n",
    "        'eval_metrics': eval_metrics\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file\n",
    "#### Change Filename here when needed!\n",
    "with open('results/results_hyperparameter_text_context_mixed.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
