{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the program used to hyperparameter tune our hyperparameters with pytorch:\n",
    "Learning Rate\n",
    "Number of Epochs\n",
    "Batch Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_text_model import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/regulafrey/Desktop/Deep_Learning/Deep_Learning_Project/Inclusiveness-in-Sarcasm-Detection\n",
      "Contents of current directory: ['text-data-preparation.ipynb', 'text-model-training.ipynb', 'requirements.txt', 'functions_text_model.py', 'audio_features.p', 'text-model-training_context.ipynb', 'functions_audio_model.py', 'results.json', 'video-model-training.ipynb', 'results_tinybert.json', 'z-old-text-model.ipynb', '__pycache__', 'README.md', 'Tryouts', '.gitignore', 'z-old-text-model-BERT-training.ipynb', 'audio_model-HuBERT.ipynb', 'hyperparameter_tuning.ipynb', 'results_tinybert_linear_added.json', 'text-model-tinybert-fallback.pth', '.git', 'data', 'hyperparameter_tuning_text_context.ipynb', 'functions_video_model.py']\n",
      "/Users/regulafrey/Desktop/Deep_Learning/Deep_Learning_Project/Inclusiveness-in-Sarcasm-Detection/data/mixed_data.json\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(\"Current directory:\", current_directory)\n",
    "\n",
    "contents = os.listdir(current_directory)\n",
    "print(\"Contents of current directory:\", contents)\n",
    "\n",
    "#### Change the Dataset here when neccessary!\n",
    "json_file_path = os.path.join(current_directory, 'data', 'mixed_data.json')\n",
    "print(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Function to encode the text\n",
    "def encode_text(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Input text\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attention masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "# PyTorch Dataset\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        utterance = item['utterance']\n",
    "        context = item['context']\n",
    "        utterance_and_context = ' '.join([sentence for sentence in context] + [utterance]) # Combining the utterance and its context into one string.\n",
    "        sarcasm = int(item['sarcasm'])\n",
    "        input_ids, attention_mask = encode_text(utterance_and_context)\n",
    "        return input_ids.flatten(), attention_mask.flatten(), sarcasm\n",
    "    \n",
    "# Create the DataLoader\n",
    "# Load the data from the JSON file\n",
    "with open(json_file_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the data to a list of dictionaries\n",
    "data = list(data.values())\n",
    "\n",
    "dataset = SarcasmDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.7033,  ACC: 0.5094, F1-weighted: 0.4456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.7008,  ACC: 0.4722, F1-weighted: 0.3448\n",
      "[{'lr': 0.001, 'batch_size': 8, 'num_epochs': 7, 'eval_loss': 0.6381482444703579, 'eval_metrics': {'ACC': 0.59375, 'F1-weighted': 0.5867649711399712}}, {'lr': 0.0001, 'batch_size': 8, 'num_epochs': 7, 'eval_loss': 0.6864803209900856, 'eval_metrics': {'ACC': 0.5279017857142857, 'F1-weighted': 0.4649926462426462}}, {'lr': 1e-05, 'batch_size': 8, 'num_epochs': 7, 'eval_loss': 0.6904398947954178, 'eval_metrics': {'ACC': 0.4966517857142857, 'F1-weighted': 0.4274825868575869}}, {'lr': 0.001, 'batch_size': 16, 'num_epochs': 7, 'eval_loss': 0.6950617507100105, 'eval_metrics': {'ACC': 0.54375, 'F1-weighted': 0.43466430610452356}}, {'lr': 0.0001, 'batch_size': 16, 'num_epochs': 7, 'eval_loss': 0.700546570122242, 'eval_metrics': {'ACC': 0.45, 'F1-weighted': 0.3761401348143339}}, {'lr': 1e-05, 'batch_size': 16, 'num_epochs': 7, 'eval_loss': 0.7107417955994606, 'eval_metrics': {'ACC': 0.44114583333333335, 'F1-weighted': 0.33333803877282137}}, {'lr': 0.001, 'batch_size': 32, 'num_epochs': 7, 'eval_loss': 0.6590756475925446, 'eval_metrics': {'ACC': 0.5670362903225806, 'F1-weighted': 0.39153668013850385}}, {'lr': 0.0001, 'batch_size': 32, 'num_epochs': 7, 'eval_loss': 0.6874293237924576, 'eval_metrics': {'ACC': 0.5504032258064516, 'F1-weighted': 0.5423988027436304}}, {'lr': 1e-05, 'batch_size': 32, 'num_epochs': 7, 'eval_loss': 0.6967290341854095, 'eval_metrics': {'ACC': 0.5272177419354839, 'F1-weighted': 0.4901515151515151}}, {'lr': 0.001, 'batch_size': 64, 'num_epochs': 7, 'eval_loss': 0.7150706350803375, 'eval_metrics': {'ACC': 0.5359623015873016, 'F1-weighted': 0.4421835978475138}}, {'lr': 0.0001, 'batch_size': 64, 'num_epochs': 7, 'eval_loss': 0.684574693441391, 'eval_metrics': {'ACC': 0.5591517857142857, 'F1-weighted': 0.55822912506444}}, {'lr': 1e-05, 'batch_size': 64, 'num_epochs': 7, 'eval_loss': 0.7007834017276764, 'eval_metrics': {'ACC': 0.4722222222222222, 'F1-weighted': 0.34479840614615825}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 1e-4, 1e-5],\n",
    "    'num_epochs': [7],\n",
    "    'batch_size': [8, 16, 32, 64]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# For each combination of hyperparameters\n",
    "for params in grid:\n",
    "    # Create a new model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels = 2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "    for parameter in model.bert.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Create a new optimizer with the current learning rate\n",
    "    optimizer = AdamW(model.classifier.parameters(), lr=params['lr'])\n",
    "    # Create the optimizer  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "    # Define the size of the training set and the test set\n",
    "    train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
    "    test_size = len(dataset) - train_size  # 20% of the data for testing\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create the DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'])\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=params['batch_size'])\n",
    "\n",
    "\n",
    "    # Train and evaluate the model for the current number of epochs\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        print('learning rate:', params['lr'], 'batch size:', params['batch_size'], 'num_epochs:', params['num_epochs'])\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_dataloader, device)\n",
    "        eval_loss, eval_metrics = evaluate(model, criterion, metrics, test_dataloader, device)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'lr': params['lr'],\n",
    "        'batch_size': params['batch_size'],\n",
    "        'num_epochs': params['num_epochs'],\n",
    "        'eval_loss': eval_loss,\n",
    "        'eval_metrics': eval_metrics\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file\n",
    "#### Change Filename here when needed!\n",
    "with open('results_hyperparameter_text_context_mixed.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
