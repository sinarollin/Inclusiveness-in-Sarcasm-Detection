{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 1\n",
    "from functions_text_model import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Function to encode the text\n",
    "def encode_text(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Input text\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attention masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "# PyTorch Dataset\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        utterance = item['utterance']\n",
    "        sarcasm = int(item['sarcasm'])\n",
    "        input_ids, attention_mask = encode_text(utterance)\n",
    "        return input_ids.flatten(), attention_mask.flatten(), sarcasm\n",
    "# Create the DataLoader\n",
    "# Load the data from the JSON file\n",
    "with open('sarcasm_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the data to a list of dictionaries\n",
    "data = list(data.values())\n",
    "\n",
    "dataset = SarcasmDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 51/69 [01:08<00:27,  1.50s/it]"
     ]
    }
   ],
   "source": [
    "#CELL 3\n",
    "torch.manual_seed(314159)\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the base BERT model\n",
    "    num_labels = 2, # Binary classification (sarcasm or not)\n",
    "    output_attentions = False, # Whether the model returns attentions weights\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states\n",
    ")\n",
    "\n",
    "for params in model.bert.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "\n",
    "#HYPEPARAMETERS found with hypertuning\n",
    "learning_rate = 0.0001\n",
    "N_EPOCHS = 5\n",
    "batchsize = 8\n",
    "\n",
    "# Define the size of the training set and the test set\n",
    "train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
    "test_size = len(dataset) - train_size  # 20% of the data for testing\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batchsize)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batchsize)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Move the model to the CPU\n",
    "model.to(device)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_metrics_log, test_metrics_log = train_cycle(model, optimizer, criterion, metrics, train_dataloader, test_dataloader, n_epochs=N_EPOCHS, device=device)\n",
    "\n",
    "# save model weights\n",
    "results_models_weights_dir = 'models_weights/'\n",
    "if not os.path.exists(results_models_weights_dir):\n",
    "    os.mkdir(results_models_weights_dir)\n",
    "torch.save(model.state_dict(), results_models_weights_dir + 'base_model_trained_on_augmented_set.pth')\n",
    "\n",
    "print('Model trained and saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "N_EPOCHS = 5\n",
    "batchsize = 8\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create a new optimizer with the current learning rate\n",
    "# FINE BUT NEED TO FOCUS ON LAYER\n",
    "optimizer = AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "# Create the optimizer  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "# Define the size of the training set and the test set\n",
    "train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
    "test_size = len(dataset) - train_size  # 20% of the data for testing\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batchsize)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batchsize)\n",
    "\n",
    "train_metrics_log, test_metrics_log = train_cycle(model, optimizer, criterion, metrics, train_dataloader, test_dataloader, n_epochs=N_EPOCHS, device=device)\n",
    "\n",
    "# save model weights\n",
    "results_models_weights_dir = 'models_weights/'\n",
    "if not os.path.exists(results_models_weights_dir):\n",
    "    os.mkdir(results_models_weights_dir)\n",
    "torch.save(model.state_dict(), results_models_weights_dir + 'base_model_trained_on_dataset.pth')\n",
    "\n",
    "print('Model trained and saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 1e-4, 1e-5],\n",
    "    'num_epochs': [5, 10, 15],\n",
    "    'batch_size': [8, 16, 32, 64]\n",
    "}\n",
    "\n",
    "# Set device\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# For each combination of hyperparameters\n",
    "for params in grid:\n",
    "    # Create a new model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels = 2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "    model.bert.parameters.requires_grad = False\n",
    "\n",
    "    #model.to(device)\n",
    "\n",
    "    # Create a new optimizer with the current learning rate\n",
    "    optimizer = AdamW(model.parameters(), lr=params['lr'])\n",
    "    # Create the optimizer  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "    # Define the size of the training set and the test set\n",
    "    train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
    "    test_size = len(dataset) - train_size  # 20% of the data for testing\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create the DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'])\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=params['batch_size'])\n",
    "\n",
    "\n",
    "    # Train and evaluate the model for the current number of epochs\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_dataloader, device)\n",
    "        eval_loss, eval_metrics = evaluate(model, criterion, metrics, test_dataloader, device)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"lr: {params['lr']}, batch_size: {params['batch_size']}, num_epochs: {params['num_epochs']}, eval_loss: {eval_loss}, eval_metrics: {eval_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(results_models_weights_dir + 'base_model_trained_on_augmented_set.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
