{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the model for text training. It is based on the code from the hyperparameter tuning, in order to replicate the results obtained in hyperparam tuning as closely as possible - did not work!! completely different results.\n",
    "Only the last layer is trained!! To train all layer remove the for loop setting require_grad to false (cell 4) and change model.classifier.parameters() to model.parameters() inside the optimizer (cell 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\celin\\anaconda3\\envs\\DLproj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functions_text_model import *\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Function to encode the text\n",
    "def encode_text(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Input text\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attention masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "# PyTorch Dataset\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        utterance = item['utterance']\n",
    "        sarcasm = int(item['sarcasm'])\n",
    "        input_ids, attention_mask = encode_text(utterance)\n",
    "        return input_ids.flatten(), attention_mask.flatten(), sarcasm\n",
    "# Create the DataLoader\n",
    "# Load the data from the JSON file\n",
    "with open('sarcasm_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the data to a list of dictionaries\n",
    "data = list(data.values())\n",
    "\n",
    "dataset = SarcasmDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6737,  ACC: 0.5679, F1-weighted: 0.5379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:08<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.6881,  ACC: 0.5125, F1-weighted: 0.4808\n",
      "train metrics {'ACC': 0.5678571428571428, 'F1-weighted': 0.5379149011116398}\n",
      "test metrics {'ACC': 0.5125, 'F1-weighted': 0.4807867742852263}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create a new model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "for parameter in model.bert.parameters():    # Remove if training the entire model\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Create a new optimizer with the current learning rate\n",
    "optimizer = AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "# Create the optimizer  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "# Define the size of the training set and the test set\n",
    "train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
    "test_size = len(dataset) - train_size  # 20% of the data for testing\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Train and evaluate the model for the current number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_dataloader, device)\n",
    "    eval_loss, eval_metrics = evaluate(model, criterion, metrics, test_dataloader, device)\n",
    "    print('train metrics', train_metrics)\n",
    "    print('test metrics', eval_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
