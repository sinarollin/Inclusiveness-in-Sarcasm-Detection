{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**: Céline Hirsch, Sandra Frey, Sina Röllin\n",
    "\n",
    "**Deep Learning Project**: Inclusiveness in Sarcasm Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_text_model import *\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to load the datasets, which have already been prepared in `text-data-preparation.ipynb`. The different datasets are the mixed, the female and the male datasets. All of these datasets were split into training, validation and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the JSON files\n",
    "with open('data/mixed_train_set.json') as file:\n",
    "    mixed_train = json.load(file)\n",
    "\n",
    "with open('data/mixed_val_set.json') as file:\n",
    "    mixed_val = json.load(file)\n",
    "\n",
    "with open('data/mixed_test_set.json') as file:\n",
    "    mixed_test = json.load(file)\n",
    "\n",
    "with open('data/M_train_set.json') as file:\n",
    "    M_train = json.load(file)\n",
    "\n",
    "with open('data/M_val_set.json') as file:\n",
    "    M_val = json.load(file)\n",
    "\n",
    "with open('data/M_test_set.json') as file:\n",
    "    M_test = json.load(file)\n",
    "\n",
    "with open('data/F_train_set.json') as file:\n",
    "    F_train = json.load(file)\n",
    "\n",
    "with open('data/F_val_set.json') as file:\n",
    "    F_val = json.load(file)\n",
    "\n",
    "with open('data/F_test_set.json') as file:\n",
    "    F_test = json.load(file)\n",
    "\n",
    "# Convert the data to a list of dictionaries\n",
    "mixed_train_data = list(mixed_train.values())\n",
    "mixed_val_data = list(mixed_val.values())\n",
    "mixed_test_data = list(mixed_test.values())\n",
    "\n",
    "M_train_data = list(M_train.values())\n",
    "M_val_data = list(M_val.values())\n",
    "M_test_data = list(M_test.values())\n",
    "\n",
    "F_train_data = list(F_train.values())\n",
    "F_val_data = list(F_val.values())\n",
    "F_test_data = list(F_test.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some classes and functions needed in the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "# Function to encode the text\n",
    "def encode_text(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Input text\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attention masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "\n",
    "# PyTorch Dataset\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        utterance = item['utterance']\n",
    "        context = item['context']\n",
    "        utterance_and_context = ' '.join([sentence for sentence in context] + [utterance]) # Combining the utterance and its context into one string.\n",
    "        sarcasm = int(item['sarcasm'])\n",
    "        input_ids, attention_mask = encode_text(utterance_and_context)\n",
    "        return input_ids.flatten(), attention_mask.flatten(), sarcasm\n",
    "    \n",
    "\n",
    "# Set seeds\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do the hyperparameter tuning for the 3 different models: \n",
    "- one model will be trained on the mixed data \n",
    "- one model will be trained on utterances from female speakers only\n",
    "- one model will be trained on utterances from male speakers only\n",
    "\n",
    "The performance of each of these models is then evaluated on the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0787,  ACC: 0.9840, F1-weighted: 0.8912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 26.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.8833,  ACC: 0.7500, F1-weighted: 0.5591\n",
      "[{'lr': 0.0001, 'batch_size': 16, 'num_epochs': 20, 'weight_decay': 0.05, 'dropout_prob': 0, 'eval_loss': 0.883301788320144, 'eval_metrics': {'ACC': 0.75, 'F1-weighted': 0.5590762860328077}}]\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETER TUNING MIXED MODEL\n",
    "\n",
    "# Set seed\n",
    "set_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "dropout_prob = 0\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 1e-4, 1e-5]\n",
    "    'num_epochs': [20],\n",
    "    'batch_size': [8, 16, 32, 64]\n",
    "    'weight_decay': [0.05, 0.1]\n",
    "    'dropout_prob': [0, 0.1]\n",
    "}\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Define the datasets\n",
    "train_dataset = SarcasmDataset(mixed_train_data)\n",
    "val_dataset = SarcasmDataset(mixed_val_data)\n",
    "\n",
    "# For each combination of hyperparameters\n",
    "for params in grid:\n",
    "\n",
    "    # Create the DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'])\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=params['batch_size'])\n",
    "\n",
    "    # Create a new model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"prajjwal1/bert-tiny\",\n",
    "        num_labels = 2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(params['dropout_prob']),\n",
    "        nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=16, out_features=2, bias=True)\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Create a new optimizer with the current learning rate\n",
    "    optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    # Create the optimizer  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define metrics\n",
    "    metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "    # Initialize lists to store losses and metrics\n",
    "    train_loss_log, test_loss_log = [], []\n",
    "    metrics_names = list(metrics.keys())\n",
    "    train_metrics_log = [[] for _ in range(len(metrics))]\n",
    "    test_metrics_log = [[] for _ in range(len(metrics))]\n",
    "\n",
    "    # Train and evaluate the model for the current number of epochs\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        print('learning rate:', params['lr'], 'batch size:', params['batch_size'], 'num_epochs:', params['num_epochs'])\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_dataloader, device)\n",
    "        val_loss, val_metrics = evaluate(model, criterion, metrics, val_dataloader, device)\n",
    "\n",
    "        # Log the losses and metrics\n",
    "        train_loss_log.append(train_loss)\n",
    "        test_loss_log.append(val_loss)\n",
    "        train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "        test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, val_metrics)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'lr': params['lr'],\n",
    "        'batch_size': params['batch_size'],\n",
    "        'num_epochs': params['num_epochs'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'dropout_prob': params['dropout_prob'],\n",
    "        'eval_loss': val_loss,\n",
    "        'eval_metrics': val_metrics\n",
    "    })\n",
    "\n",
    "    # Plot and save the training and testing metrics\n",
    "    plot_filename = 'plot-test' #f'hyperparameter_tuning/plot_lr_{params[\"lr\"]}_bs_{params[\"batch_size\"]}_wd_{params[\"weight_decay\"]}_dp_{params[\"dropout_prob\"]}.png'\n",
    "    plot_training_hyperparameters(train_loss_log, test_loss_log, metrics_names, train_metrics_log, test_metrics_log, plot_filename)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "results_path = 'hyperparameter_tuning/text_hyperparameter_mixed.json'\n",
    "os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5340,  ACC: 0.7685, F1-weighted: 0.5081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.6841,  ACC: 0.5870, F1-weighted: 0.5852\n",
      "[{'lr': 0.0001, 'batch_size': 8, 'num_epochs': 20, 'weight_decay': 0.05, 'dropout_prob': 0, 'eval_loss': 0.6725867787996928, 'eval_metrics': {'ACC': 0.5833333333333334, 'F1-weighted': 0.5095571095571095}}, {'lr': 0.0001, 'batch_size': 8, 'num_epochs': 20, 'weight_decay': 0.1, 'dropout_prob': 0, 'eval_loss': 0.6949547131856283, 'eval_metrics': {'ACC': 0.4791666666666667, 'F1-weighted': 0.41111111111111115}}, {'lr': 0.0001, 'batch_size': 8, 'num_epochs': 20, 'weight_decay': 0.05, 'dropout_prob': 0.1, 'eval_loss': 0.6696785887082418, 'eval_metrics': {'ACC': 0.6180555555555556, 'F1-weighted': 0.37917637917637914}}, {'lr': 0.0001, 'batch_size': 8, 'num_epochs': 20, 'weight_decay': 0.1, 'dropout_prob': 0.1, 'eval_loss': 0.7038969000180563, 'eval_metrics': {'ACC': 0.5972222222222222, 'F1-weighted': 0.44983164983164986}}, {'lr': 0.0001, 'batch_size': 16, 'num_epochs': 20, 'weight_decay': 0.05, 'dropout_prob': 0, 'eval_loss': 1.153458038965861, 'eval_metrics': {'ACC': 0.5684523809523809, 'F1-weighted': 0.4656574546734729}}, {'lr': 0.0001, 'batch_size': 16, 'num_epochs': 20, 'weight_decay': 0.1, 'dropout_prob': 0, 'eval_loss': 0.767370750506719, 'eval_metrics': {'ACC': 0.7023809523809524, 'F1-weighted': 0.5193646932777368}}, {'lr': 0.0001, 'batch_size': 16, 'num_epochs': 20, 'weight_decay': 0.05, 'dropout_prob': 0.1, 'eval_loss': 0.9154331286748251, 'eval_metrics': {'ACC': 0.6071428571428571, 'F1-weighted': 0.4932496318685577}}, {'lr': 0.0001, 'batch_size': 16, 'num_epochs': 20, 'weight_decay': 0.1, 'dropout_prob': 0.1, 'eval_loss': 0.9800216555595398, 'eval_metrics': {'ACC': 0.5595238095238095, 'F1-weighted': 0.4720375106564365}}, {'lr': 0.0001, 'batch_size': 32, 'num_epochs': 20, 'weight_decay': 0.05, 'dropout_prob': 0, 'eval_loss': 0.8528240025043488, 'eval_metrics': {'ACC': 0.5669642857142857, 'F1-weighted': 0.4595959595959596}}, {'lr': 0.0001, 'batch_size': 32, 'num_epochs': 20, 'weight_decay': 0.1, 'dropout_prob': 0, 'eval_loss': 0.6904868334531784, 'eval_metrics': {'ACC': 0.6584821428571428, 'F1-weighted': 0.48539589442815245}}, {'lr': 0.0001, 'batch_size': 32, 'num_epochs': 20, 'weight_decay': 0.05, 'dropout_prob': 0.1, 'eval_loss': 0.777431845664978, 'eval_metrics': {'ACC': 0.6183035714285714, 'F1-weighted': 0.4874439657048353}}, {'lr': 0.0001, 'batch_size': 32, 'num_epochs': 20, 'weight_decay': 0.1, 'dropout_prob': 0.1, 'eval_loss': 0.6372337639331818, 'eval_metrics': {'ACC': 0.7053571428571428, 'F1-weighted': 0.5317647058823529}}, {'lr': 0.0001, 'batch_size': 64, 'num_epochs': 20, 'weight_decay': 0.05, 'dropout_prob': 0, 'eval_loss': 0.6764252781867981, 'eval_metrics': {'ACC': 0.6304347826086957, 'F1-weighted': 0.6288561936402468}}, {'lr': 0.0001, 'batch_size': 64, 'num_epochs': 20, 'weight_decay': 0.1, 'dropout_prob': 0, 'eval_loss': 0.6817560791969299, 'eval_metrics': {'ACC': 0.5652173913043478, 'F1-weighted': 0.5652173913043478}}, {'lr': 0.0001, 'batch_size': 64, 'num_epochs': 20, 'weight_decay': 0.05, 'dropout_prob': 0.1, 'eval_loss': 0.7054293155670166, 'eval_metrics': {'ACC': 0.6086956521739131, 'F1-weighted': 0.5964912280701754}}, {'lr': 0.0001, 'batch_size': 64, 'num_epochs': 20, 'weight_decay': 0.1, 'dropout_prob': 0.1, 'eval_loss': 0.6841440796852112, 'eval_metrics': {'ACC': 0.5869565217391305, 'F1-weighted': 0.5851922164214522}}]\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETER TUNING MALE MODEL\n",
    "\n",
    "set_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "dropout_prob = 0\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'lr': [1e-4],\n",
    "    'num_epochs': [20],\n",
    "    'batch_size': [8, 16, 32, 64],\n",
    "    'weight_decay': [0.05, 0.1],\n",
    "    'dropout_prob': [0, 0.1]\n",
    "}\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Define the datasets\n",
    "train_dataset = SarcasmDataset(M_train_data)\n",
    "val_dataset = SarcasmDataset(M_val_data)\n",
    "\n",
    "# For each combination of hyperparameters\n",
    "for params in grid:\n",
    "\n",
    "    # Create the DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'])\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=params['batch_size'])\n",
    "\n",
    "    # Create a new model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"prajjwal1/bert-tiny\",\n",
    "        num_labels = 2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(params['dropout_prob']),\n",
    "        nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=16, out_features=2, bias=True)\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Create a new optimizer with the current learning rate\n",
    "    optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    # Create the optimizer  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define metrics\n",
    "    metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "    # Initialize lists to store losses and metrics\n",
    "    train_loss_log, test_loss_log = [], []\n",
    "    metrics_names = list(metrics.keys())\n",
    "    train_metrics_log = [[] for _ in range(len(metrics))]\n",
    "    test_metrics_log = [[] for _ in range(len(metrics))]\n",
    "\n",
    "    # Train and evaluate the model for the current number of epochs\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        print('learning rate:', params['lr'], 'batch size:', params['batch_size'], 'num_epochs:', params['num_epochs'])\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_dataloader, device)\n",
    "        val_loss, val_metrics = evaluate(model, criterion, metrics, val_dataloader, device)\n",
    "\n",
    "        # Log the losses and metrics\n",
    "        train_loss_log.append(train_loss)\n",
    "        test_loss_log.append(val_loss)\n",
    "        train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "        test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, val_metrics)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'lr': params['lr'],\n",
    "        'batch_size': params['batch_size'],\n",
    "        'num_epochs': params['num_epochs'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'dropout_prob': params['dropout_prob'],\n",
    "        'eval_loss': val_loss,\n",
    "        'eval_metrics': val_metrics\n",
    "    })\n",
    "\n",
    "    # Plot and save the training and testing metrics\n",
    "    plot_filename = f'hyperparameter_tuning/M_plot_lr_{params[\"lr\"]}_bs_{params[\"batch_size\"]}_wd_{params[\"weight_decay\"]}_dp_{params[\"dropout_prob\"]}.png'\n",
    "    plot_training_hyperparameters(train_loss_log, test_loss_log, metrics_names, train_metrics_log, test_metrics_log, plot_filename)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "results_path = 'hyperparameter_tuning/text_hyperparameter_M.json'\n",
    "os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6328,  ACC: 0.6302, F1-weighted: 0.4904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.8266,  ACC: 0.4074, F1-weighted: 0.2895\n",
      "[{'lr': 0.0001, 'batch_size': 8, 'num_epochs': 10, 'weight_decay': 0.05, 'dropout_prob': 0, 'eval_loss': 0.5999684780836105, 'eval_metrics': {'ACC': 0.65625, 'F1-weighted': 0.5086580086580086}}, {'lr': 0.0001, 'batch_size': 8, 'num_epochs': 10, 'weight_decay': 0.1, 'dropout_prob': 0, 'eval_loss': 0.6194020807743073, 'eval_metrics': {'ACC': 0.6875, 'F1-weighted': 0.5181818181818182}}, {'lr': 0.0001, 'batch_size': 8, 'num_epochs': 10, 'weight_decay': 0.05, 'dropout_prob': 0.1, 'eval_loss': 0.5742475241422653, 'eval_metrics': {'ACC': 0.75, 'F1-weighted': 0.5461538461538462}}, {'lr': 0.0001, 'batch_size': 8, 'num_epochs': 10, 'weight_decay': 0.1, 'dropout_prob': 0.1, 'eval_loss': 0.6570921838283539, 'eval_metrics': {'ACC': 0.625, 'F1-weighted': 0.48484848484848486}}, {'lr': 0.0001, 'batch_size': 16, 'num_epochs': 10, 'weight_decay': 0.05, 'dropout_prob': 0, 'eval_loss': 0.7052564173936844, 'eval_metrics': {'ACC': 0.5340909090909092, 'F1-weighted': 0.325}}, {'lr': 0.0001, 'batch_size': 16, 'num_epochs': 10, 'weight_decay': 0.1, 'dropout_prob': 0, 'eval_loss': 0.6513199359178543, 'eval_metrics': {'ACC': 0.5965909090909092, 'F1-weighted': 0.36136363636363633}}, {'lr': 0.0001, 'batch_size': 16, 'num_epochs': 10, 'weight_decay': 0.05, 'dropout_prob': 0.1, 'eval_loss': 0.6559459120035172, 'eval_metrics': {'ACC': 0.6107954545454546, 'F1-weighted': 0.3571428571428571}}, {'lr': 0.0001, 'batch_size': 16, 'num_epochs': 10, 'weight_decay': 0.1, 'dropout_prob': 0.1, 'eval_loss': 0.6616234183311462, 'eval_metrics': {'ACC': 0.6278409090909092, 'F1-weighted': 0.37717391304347825}}, {'lr': 0.0001, 'batch_size': 32, 'num_epochs': 10, 'weight_decay': 0.05, 'dropout_prob': 0, 'eval_loss': 0.7008503079414368, 'eval_metrics': {'ACC': 0.5185185185185185, 'F1-weighted': 0.4935064935064935}}, {'lr': 0.0001, 'batch_size': 32, 'num_epochs': 10, 'weight_decay': 0.1, 'dropout_prob': 0, 'eval_loss': 0.7798951268196106, 'eval_metrics': {'ACC': 0.48148148148148145, 'F1-weighted': 0.4166666666666667}}, {'lr': 0.0001, 'batch_size': 32, 'num_epochs': 10, 'weight_decay': 0.05, 'dropout_prob': 0.1, 'eval_loss': 0.7355891466140747, 'eval_metrics': {'ACC': 0.4444444444444444, 'F1-weighted': 0.35612082670906203}}, {'lr': 0.0001, 'batch_size': 32, 'num_epochs': 10, 'weight_decay': 0.1, 'dropout_prob': 0.1, 'eval_loss': 0.7441558837890625, 'eval_metrics': {'ACC': 0.48148148148148145, 'F1-weighted': 0.4166666666666667}}, {'lr': 0.0001, 'batch_size': 64, 'num_epochs': 10, 'weight_decay': 0.05, 'dropout_prob': 0, 'eval_loss': 1.0187605619430542, 'eval_metrics': {'ACC': 0.4074074074074074, 'F1-weighted': 0.2894736842105263}}, {'lr': 0.0001, 'batch_size': 64, 'num_epochs': 10, 'weight_decay': 0.1, 'dropout_prob': 0, 'eval_loss': 0.7108497023582458, 'eval_metrics': {'ACC': 0.5185185185185185, 'F1-weighted': 0.47218045112781953}}, {'lr': 0.0001, 'batch_size': 64, 'num_epochs': 10, 'weight_decay': 0.05, 'dropout_prob': 0.1, 'eval_loss': 0.8861312866210938, 'eval_metrics': {'ACC': 0.4074074074074074, 'F1-weighted': 0.2894736842105263}}, {'lr': 0.0001, 'batch_size': 64, 'num_epochs': 10, 'weight_decay': 0.1, 'dropout_prob': 0.1, 'eval_loss': 0.8265929818153381, 'eval_metrics': {'ACC': 0.4074074074074074, 'F1-weighted': 0.2894736842105263}}]\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETER TUNING FEMALE MODEL\n",
    "\n",
    "set_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "dropout_prob = 0\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'lr': [1e-4],\n",
    "    'num_epochs': [10],\n",
    "    'batch_size': [8, 16, 32, 64],\n",
    "    'weight_decay': [0.05, 0.1],\n",
    "    'dropout_prob': [0, 0.1]\n",
    "}\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Define the datasets\n",
    "train_dataset = SarcasmDataset(F_train_data)\n",
    "val_dataset = SarcasmDataset(F_val_data)\n",
    "\n",
    "# For each combination of hyperparameters\n",
    "for params in grid:\n",
    "\n",
    "    # Create the DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'], worker_init_fn=lambda worker_id: set_seed(42))\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=params['batch_size'])\n",
    "\n",
    "    # Create a new model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"prajjwal1/bert-tiny\",\n",
    "        num_labels = 2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(params['dropout_prob']),\n",
    "        nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=16, out_features=2, bias=True)\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Create a new optimizer with the current learning rate\n",
    "    optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    # Create the optimizer  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define metrics\n",
    "    metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "    # Initialize lists to store losses and metrics\n",
    "    train_loss_log, test_loss_log = [], []\n",
    "    metrics_names = list(metrics.keys())\n",
    "    train_metrics_log = [[] for _ in range(len(metrics))]\n",
    "    test_metrics_log = [[] for _ in range(len(metrics))]\n",
    "\n",
    "    # Train and evaluate the model for the current number of epochs\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        print('learning rate:', params['lr'], 'batch size:', params['batch_size'], 'num_epochs:', params['num_epochs'])\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_dataloader, device)\n",
    "        val_loss, val_metrics = evaluate(model, criterion, metrics, val_dataloader, device)\n",
    "\n",
    "        # Log the losses and metrics\n",
    "        train_loss_log.append(train_loss)\n",
    "        test_loss_log.append(val_loss)\n",
    "        train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "        test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, val_metrics)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'lr': params['lr'],\n",
    "        'batch_size': params['batch_size'],\n",
    "        'num_epochs': params['num_epochs'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'dropout_prob': params['dropout_prob'],\n",
    "        'eval_loss': val_loss,\n",
    "        'eval_metrics': val_metrics\n",
    "    })\n",
    "\n",
    "    # Plot and save the training and testing metrics\n",
    "    plot_filename = f'hyperparameter_tuning/F_plot_lr_{params[\"lr\"]}_bs_{params[\"batch_size\"]}_wd_{params[\"weight_decay\"]}_dp_{params[\"dropout_prob\"]}.png'\n",
    "    plot_training_hyperparameters(train_loss_log, test_loss_log, metrics_names, train_metrics_log, test_metrics_log, plot_filename)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "results_path = 'hyperparameter_tuning/text_hyperparameter_F.json'\n",
    "os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the different models with different hyperparameters and evaluated them on the validation sets, we can choose which hyperparameters give us the best model.\n",
    "\n",
    "We then train those models again with the number of epochs we chose from the graphs. That way we can save the models to access them again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0725,  ACC: 0.9853, F1-weighted: 0.9031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.8306,  ACC: 0.7708, F1-weighted: 0.5797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MIXED MODEL WITH CHOSEN HYPERPARAMETERS\n",
    "\n",
    "set_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.0001\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "weight_decay = 0.1\n",
    "dropout_prob = 0\n",
    "\n",
    "# Define the datasets\n",
    "train_dataset = SarcasmDataset(mixed_train_data)\n",
    "val_dataset = SarcasmDataset(mixed_val_data)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Create a new model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\",\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(dropout_prob),\n",
    "    nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=16, out_features=2, bias=True)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Create a new optimizer with the current learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Create the optimizer  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define metrics\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "# Initialize lists to store losses and metrics\n",
    "train_loss_log, test_loss_log = [], []\n",
    "metrics_names = list(metrics.keys())\n",
    "train_metrics_log = [[] for _ in range(len(metrics))]\n",
    "test_metrics_log = [[] for _ in range(len(metrics))]\n",
    "\n",
    "# Train and evaluate the model for the current number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_dataloader, device)\n",
    "    val_loss, val_metrics = evaluate(model, criterion, metrics, val_dataloader, device)\n",
    "\n",
    "    # Log the losses and metrics\n",
    "    train_loss_log.append(train_loss)\n",
    "    test_loss_log.append(val_loss)\n",
    "    train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "    test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, val_metrics)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'models/mixed_model_text.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6388,  ACC: 0.6222, F1-weighted: 0.4687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 39.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.6557,  ACC: 0.6250, F1-weighted: 0.5232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MALE MODEL WITH CHOSEN HYPERPARAMETERS\n",
    "\n",
    "set_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.0001\n",
    "num_epochs = 20\n",
    "batch_size = 8\n",
    "weight_decay = 0.1\n",
    "dropout_prob = 0\n",
    "\n",
    "# Define the datasets\n",
    "train_dataset = SarcasmDataset(M_train_data)\n",
    "val_dataset = SarcasmDataset(M_val_data)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Create a new model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\",\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(dropout_prob),\n",
    "    nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=16, out_features=2, bias=True)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Create a new optimizer with the current learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Create the optimizer  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define metrics\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "# Initialize lists to store losses and metrics\n",
    "train_loss_log, test_loss_log = [], []\n",
    "metrics_names = list(metrics.keys())\n",
    "train_metrics_log = [[] for _ in range(len(metrics))]\n",
    "test_metrics_log = [[] for _ in range(len(metrics))]\n",
    "\n",
    "# Train and evaluate the model for the current number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_dataloader, device)\n",
    "    val_loss, val_metrics = evaluate(model, criterion, metrics, val_dataloader, device)\n",
    "\n",
    "    # Log the losses and metrics\n",
    "    train_loss_log.append(train_loss)\n",
    "    test_loss_log.append(val_loss)\n",
    "    train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "    test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, val_metrics)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'models/M_model_text.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5139,  ACC: 0.8289, F1-weighted: 0.6225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 44.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.5974,  ACC: 0.7500, F1-weighted: 0.6643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN FEMALE MODEL WITH CHOSEN HYPERPARAMETERS\n",
    "\n",
    "set_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.0001\n",
    "num_epochs = 9\n",
    "batch_size = 8\n",
    "weight_decay = 0.05\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# Define the datasets\n",
    "train_dataset = SarcasmDataset(F_train_data)\n",
    "val_dataset = SarcasmDataset(F_val_data)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, worker_init_fn=lambda worker_id: set_seed(42))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Create a new model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\",\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(dropout_prob),\n",
    "    nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=16, out_features=2, bias=True)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Create a new optimizer with the current learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Create the optimizer  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define metrics\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "# Initialize lists to store losses and metrics\n",
    "train_loss_log, test_loss_log = [], []\n",
    "metrics_names = list(metrics.keys())\n",
    "train_metrics_log = [[] for _ in range(len(metrics))]\n",
    "test_metrics_log = [[] for _ in range(len(metrics))]\n",
    "\n",
    "# Train and evaluate the model for the current number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_dataloader, device)\n",
    "    val_loss, val_metrics = evaluate(model, criterion, metrics, val_dataloader, device)\n",
    "\n",
    "    # Log the losses and metrics\n",
    "    train_loss_log.append(train_loss)\n",
    "    test_loss_log.append(val_loss)\n",
    "    train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "    test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, val_metrics)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'models/F_model_text.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have chosen the best hyperparameters for each model and saved the model weights, we will evaluate the model performances on the untouched test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 6/6 [00:00<00:00, 30.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.5574,  ACC: 0.8507, F1-weighted: 0.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test mixed model performance on test set\n",
    "\n",
    "set_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 16\n",
    "dropout_prob = 0\n",
    "\n",
    "# Define the dataset\n",
    "test_dataset = SarcasmDataset(mixed_test_data)\n",
    "\n",
    "# Create the DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Load the model\n",
    "mixed_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "mixed_model.classifier = nn.Sequential(\n",
    "    nn.Dropout(dropout_prob),\n",
    "    nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=16, out_features=2, bias=True)\n",
    "    )\n",
    "\n",
    "# Load the weights\n",
    "state_dict = torch.load(\"models/mixed_model_text.pth\")\n",
    "mixed_model.load_state_dict(state_dict)\n",
    "\n",
    "mixed_model.eval()\n",
    "mixed_model.to(device)\n",
    "\n",
    "# Define the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the metrics\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_metrics = evaluate(mixed_model, criterion, metrics, test_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:00<00:00, 14.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.6473,  ACC: 0.7240, F1-weighted: 0.5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test male model performance on test set\n",
    "\n",
    "set_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 32\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# Define the dataset\n",
    "test_dataset = SarcasmDataset(M_test_data)\n",
    "\n",
    "# Create the DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(dropout_prob),\n",
    "    nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=16, out_features=2, bias=True)\n",
    "    )\n",
    "\n",
    "# Load the weights\n",
    "state_dict = torch.load(\"models/M_model_text.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the metrics\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_metrics = evaluate(model, criterion, metrics, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:00<00:00, 39.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval Loss: 0.6024,  ACC: 0.7188, F1-weighted: 0.6522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test female model performance on test set\n",
    "\n",
    "set_seed(42)\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 16\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# Define the dataset\n",
    "test_dataset = SarcasmDataset(F_test_data)\n",
    "\n",
    "# Create the DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(dropout_prob),\n",
    "    nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=16, out_features=2, bias=True)\n",
    "    )\n",
    "\n",
    "# Load the weights\n",
    "state_dict = torch.load(\"models/F_model_text.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the metrics\n",
    "metrics = {'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_metrics = evaluate(model, criterion, metrics, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
